{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD TRAINING AND TESTING (TUTORIAL)\n",
    "In this tutorial, we will show how to initialize, train and test an SSD300 for object detection.\n",
    "The tutorial is based on a subset of pictures from the PascalVOC dataset (both 2007 and 2012)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS\n",
    "We start by importing all the necessary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from smithers.ml.vgg import VGG\n",
    "from smithers.ml.models.aux_conv import AuxiliaryConvolutions\n",
    "from smithers.ml.models.predictor import PredictionConvolutions\n",
    "from smithers.ml.dataset.pascalvoc_dataset import PascalVOCDataset\n",
    "from smithers.ml.models.detector import Detector\n",
    "from smithers.ml.models.utils import create_prior_boxes\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEARNING PARAMETERS\n",
    "Then, we set the parameters used for the data, the detector and the learning phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories: {'cat': 1, 'dog': 2, 'background': 0}\n",
      "n_classes: 3\n"
     ]
    }
   ],
   "source": [
    "# Learning parameters\n",
    "batch_size = 8  # batch size\n",
    "workers = 4  # number of workers for loading data in the DataLoader\n",
    "iterations = 120000  # number of iterations to train\n",
    "print_freq = 200  # print training status every __ batches\n",
    "lr = 1e-4  # learning rate\n",
    "decay_lr_at = [80000, 100000]  # decay learning rate after these many iterations\n",
    "decay_lr_to = 0.1\n",
    "# decay learning rate to this fraction of the existing learning rate\n",
    "#n_classes = 6\n",
    "momentum = 0.9  # momentum\n",
    "weight_decay = 5e-4  # weight decay\n",
    "grad_clip = None\n",
    "# clip if gradients are exploding, which may happen at larger batch sizes\n",
    "\n",
    "#voc_labels = ('aeroplane', 'bicycle', 'bird', 'boat',\n",
    "#        'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "#        'cow', 'diningtable', 'dog', 'horse',\n",
    "#        'motorbike', 'person', 'pottedplant',\n",
    "#        'sheep', 'sofa', 'train', 'tvmonitor')\n",
    "voc_labels = ('cat', 'dog')\n",
    "label_map = {k: v + 1 for v, k in enumerate(voc_labels)}\n",
    "label_map['background'] = 0\n",
    "n_classes = len(label_map)\n",
    "print('categories:',label_map)\n",
    "print('n_classes:', n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PARAMETERS\n",
    "We now define the train and test images, after they have been extracted and the subdivision has been carried out (refer to the data preparation tutorial for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 240\n",
      "Testing images: 60\n"
     ]
    }
   ],
   "source": [
    "# Data parameters\n",
    "data_folder = 'VOC_dog_cat/JSONfiles' #folder with json data files\n",
    "keep_difficult = True\n",
    "\n",
    "\n",
    "train_dataset = PascalVOCDataset(data_folder,\n",
    "                                 split='train',\n",
    "                                 keep_difficult=keep_difficult)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=train_dataset.collate_fn,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True)\n",
    "\n",
    "epochs = iterations // (len(train_dataset) // 16) #500\n",
    "decay_lr_at = [it // (len(train_dataset) // 16) for it in decay_lr_at]\n",
    "print('Training images:', len(train_dataset))\n",
    "\n",
    "\n",
    "test_dataset = PascalVOCDataset(data_folder,\n",
    "                                split='test',\n",
    "                                keep_difficult=keep_difficult)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          collate_fn=test_dataset.collate_fn,\n",
    "                                          num_workers=workers,\n",
    "                                          pin_memory=True)\n",
    "print('Testing images:', len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NETWORK INITIALIZATION\n",
    "The following cell contains the lines used to instantiate the detector object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded base model.\n",
      "\n",
      "Time needed to initialize the net: 1.85 seconds\n"
     ]
    }
   ],
   "source": [
    "start_init = time()\n",
    "base_net = VGG(classifier='ssd', init_weights=False)\n",
    "aux_conv = AuxiliaryConvolutions()\n",
    "predictor = PredictionConvolutions(n_classes)\n",
    "network = [base_net, aux_conv, predictor]\n",
    "priors_cxcy = create_prior_boxes()\n",
    "end_init = time()\n",
    "print(f'Time needed to initialize the net: {round(end_init-start_init,2)} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NETWORK TRAINING/LOADING A CHECKPOINT, TESTING THE NETWORK\n",
    "Here, the network will trained and, if provided, a checkpoint can be used to start from an already trained network.\n",
    "It is possible to choose a different number for the ``epochs`` variable (obviously, choosing a low number will not produce satisfactory results).\n",
    "The cell also prints the time spans needed to perform some operations and prints the final model, moreover pieces of information specific to the different epochs are generated at each iteration.\n",
    "\n",
    "The training procedure will store a checkpoint file named ``checkpoint_ssd300.pth.tar``, once it is completed.\n",
    "Furthermore, the plot of the value of the loss against the corresponding epoch is produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded checkpoint from epoch 500.\n",
      "\n",
      "[VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
      "    (1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "), AuxiliaryConvolutions(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (4): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (6): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "), PredictionConvolutions(\n",
      "  (features_loc): Sequential(\n",
      "    (0): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (features_cl): Sequential(\n",
      "    (0): Conv2d(512, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(1024, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(512, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): Conv2d(256, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): Conv2d(256, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")]\n",
      "Training has started.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Adam' object has no attribute '_warned_capturable_if_run_uncaptured'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/u/s/szanin/Smithers/smithers/ml/tutorials/training_SSD.ipynb Cella 11\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blovelace/u/s/szanin/Smithers/smithers/ml/tutorials/training_SSD.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m detector \u001b[39m=\u001b[39m Detector(network, checkpoint, priors_cxcy, n_classes, epochs,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blovelace/u/s/szanin/Smithers/smithers/ml/tutorials/training_SSD.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m                         batch_size, print_freq, lr, decay_lr_at,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blovelace/u/s/szanin/Smithers/smithers/ml/tutorials/training_SSD.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m                         decay_lr_to, momentum, weight_decay, grad_clip,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blovelace/u/s/szanin/Smithers/smithers/ml/tutorials/training_SSD.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m                         train_loader, test_loader)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blovelace/u/s/szanin/Smithers/smithers/ml/tutorials/training_SSD.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(detector\u001b[39m.\u001b[39mmodel)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blovelace/u/s/szanin/Smithers/smithers/ml/tutorials/training_SSD.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m checkpoint, loss_value \u001b[39m=\u001b[39m detector\u001b[39m.\u001b[39;49mtrain_detector()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blovelace/u/s/szanin/Smithers/smithers/ml/tutorials/training_SSD.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCHECKPOINT: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m- type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(checkpoint)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m- Name: \u001b[39m\u001b[39m{\u001b[39;00mcheckpoint\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blovelace/u/s/szanin/Smithers/smithers/ml/tutorials/training_SSD.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m end \u001b[39m=\u001b[39m time()\n",
      "File \u001b[0;32m~/Smithers/smithers/ml/models/detector.py:290\u001b[0m, in \u001b[0;36mDetector.train_detector\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m         adjust_learning_rate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecay_lr_to)\n\u001b[1;32m    289\u001b[0m     \u001b[39m# One epoch's training\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m     loss_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_epoch(epoch\u001b[39m=\u001b[39;49mepoch)\n\u001b[1;32m    291\u001b[0m     loss_values\u001b[39m.\u001b[39mextend([loss_val])\n\u001b[1;32m    293\u001b[0m \u001b[39m# Save checkpoint\u001b[39;00m\n",
      "File \u001b[0;32m~/Smithers/smithers/ml/models/detector.py:253\u001b[0m, in \u001b[0;36mDetector.train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    250\u001b[0m     clip_gradient(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad_clip)\n\u001b[1;32m    252\u001b[0m \u001b[39m# Update model\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    255\u001b[0m losses\u001b[39m.\u001b[39mupdate(loss\u001b[39m.\u001b[39mitem(), images\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\n\u001b[1;32m    256\u001b[0m batch_time\u001b[39m.\u001b[39mupdate(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start)\n",
      "File \u001b[0;32m~/miniconda3/envs/reducedcnn/lib/python3.10/site-packages/torch/optim/optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 109\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/reducedcnn/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/reducedcnn/lib/python3.10/site-packages/torch/optim/adam.py:113\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, closure\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    107\u001b[0m     \u001b[39m\"\"\"Performs a single optimization step.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \n\u001b[1;32m    109\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39m        closure (callable, optional): A closure that reevaluates the model\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m            and returns the loss.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cuda_graph_capture_health_check()\n\u001b[1;32m    115\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/reducedcnn/lib/python3.10/site-packages/torch/optim/optimizer.py:93\u001b[0m, in \u001b[0;36mOptimizer._cuda_graph_capture_health_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mif\u001b[39;00m capturing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mcapturable\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     89\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAttempting CUDA graph capture of step() for an instance of \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[1;32m     90\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m+\u001b[39m\n\u001b[1;32m     91\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39m but this instance was constructed with capturable=False.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_warned_capturable_if_run_uncaptured) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mcapturable\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m capturing):\n\u001b[1;32m     94\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mWarning: This instance was constructed with capturable=True, but step() \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[1;32m     95\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mis running without CUDA graph capture. If you never intend to graph-capture this \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[1;32m     96\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39minstance, capturable=True can impair performance, and you should set capturable=False.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Adam' object has no attribute '_warned_capturable_if_run_uncaptured'"
     ]
    }
   ],
   "source": [
    "#checkpoint = None # if no checkpoint available\n",
    "checkpoint = 'checkpoint_ssd300.pth.tar' #if a checkpoint is available\n",
    "epochs = 500\n",
    "start = time()\n",
    "detector = Detector(network, checkpoint, priors_cxcy, n_classes, epochs,\n",
    "                        batch_size, print_freq, lr, decay_lr_at,\n",
    "                        decay_lr_to, momentum, weight_decay, grad_clip,\n",
    "                        train_loader, test_loader)\n",
    "print(detector.model)\n",
    "checkpoint, loss_value = detector.train_detector()\n",
    "print(f'CHECKPOINT: \\n- type: {type(checkpoint)}\\n- Name: {checkpoint}')\n",
    "end = time()\n",
    "print(f'Time needed for training the net: {round(end-start,2)} seconds, i.e. {round((end-start)/60,1)} minutes')\n",
    "start_test = time()\n",
    "epo = np.arange(start=0, stop=epochs, step=1)\n",
    "plt.plot(epo, loss_value)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Value Loss')\n",
    "plt.savefig('loss_pascal_cat.png')\n",
    "detector.eval_detector(label_map, checkpoint)\n",
    "end_test = time()\n",
    "\n",
    "print(f'Time needed for testing the net: {round(end_test-start_test,2)} seconds, i.e. {round((end_test-start_test)/60,1)} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING WITH AN IMAGE + PRINT\n",
    "By choosing a specific image from the directory used, the network is able to detect the object(s) in it and save a version of the image in which the bounding box(es) with the relative label(s) is(/are) pictured. To view this, open the file ``out.jpg``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/u/s/szanin/Smithers/smithers/ml/tutorials/VOC_dog_cat/JPEGImages/002215.jpg'\n",
    "#img_path = 'voc_dir/VOC_cat-dog/JPEGImages/000122.jpg'\n",
    "original_image = Image.open(img_path, mode='r')\n",
    "original_image = original_image.convert('RGB')\n",
    "\n",
    "detector.detect(original_image,\n",
    "                checkpoint,\n",
    "                label_map,\n",
    "                min_score=0.01,\n",
    "                max_overlap=0.45,\n",
    "                top_k=5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('reducedcnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c5bf16c94eb6f9341fa612a12f652937166e39821fa969ec7095b77ab48ffd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
