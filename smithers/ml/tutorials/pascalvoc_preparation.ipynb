{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to prepare PascalVOC Dataset to train an object detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data\n",
    "The PascalVOC Dataset used is composed of two different datasets from the years 2007 and 2012: VOC2007 and VOC2012. First of all you need to download the datasets from the official webpages:\n",
    "1. VOC2007: from http://host.robots.ox.ac.uk/pascal/VOC/voc2007/index.html#devkit select ''Download the training/validation data (450MB tar file)'' in the Development Kit section and ''Download the annotated test data (430MB tar file)'' in the Test Data section.\n",
    "2. VOC2012: from http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html select ''Download the training/validation data (2GB tar file)'' in the Development Kit section.\n",
    "\n",
    "\n",
    "The two trainval datasets, downloaded from the Development Kit section, are to be used for training, while the VOC 2007 test, the one taken from the Test Data section, will serve as test dataset.\n",
    "\n",
    "ATTENTION: Both the VOC2007 trainval and VOC2007 test data has to be extracted in the same location, e.g. download the datasets and then merge them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of the Dataset folder - Pascal VOC \n",
    "Once you have downloaded the aforementioned datasets, you should place them in the same folder. Hence, inside the main folder 'VOCdevkit'there should be two subfolders 'VOC2007' and 'VOC2012', where each of them contains five subfolders: \n",
    "1. Annotations: Inside this folder there are the PascalVOC formatted annotation XML files, that contain relevant information for the picture under examination. Hence, there is one XML file per image. Each XML file contains the path to the image in the 'path' element, the bounding box stored in an 'object' element and other features as can be seen in the example below. You can note that the bounding box is defined by two points, the upper left and bottom right corners.\n",
    "<img src = \"pascalvoc.PNG\" style = \"height:400px\">\n",
    "\n",
    "2. ImageSets: Inside this folder there are three subfolders: 'Layout', 'Main' and 'Segmentation'. In particular in the subfolder 'Main' you can find the images of a specific class that belong to the test, train or trainval subdivision.\n",
    "3. JPEGImages: Here there are all the images, that has to be in the JPG format.\n",
    "4. and 5. SegmentationClass and SegmentationObject: folders containing the segmentation masks for some images and objects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline\n",
    "Once you have the correct structure of the dataset, you should divide data into training and test splits. Data should also be saved in JSON files in order to be used inside the PyTorch Dataset class that will be created later for this purpose.\n",
    "\n",
    "### Parse raw data - Creation JSON files & splitting of the dataset\n",
    "Run (and see for more details) the create_json.py script you can find in the dataset folder. When running it, you need to provide the paths to the VOC2007 and VOC2012 folders, as well as to the desired output folder where the JSON files should be saved.\n",
    "\n",
    "This script parses the data downloaded and returns as output the following files:\n",
    "1. A JSON file for each split (Train or Test) with a list of the absolute filepaths for each image in that split.\n",
    "2. A JSON file for each split (train or Test) with a list of dictionaries containing ground truth objects, i.e. bounding boxes in absolute boundary coordinates, their encoded labels, and perceived detection difficulties for each image in that split. Therefore, The i-th dictionary in this list will contain the objects present in the i-th image of the split.\n",
    "3. A JSON file which contains the label_map, the label-to-index dictionary with which the labels are encoded in the previous JSON file. This dictionary is also available in the script (create_json.py) and directly importable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/u/s/szanin/Smithers/smithers/ml/VOCdevkit/VOC2007\n",
      "/u/s/szanin/Smithers/smithers/ml/VOCdevkit/VOC2007\n",
      "/u/s/szanin/Smithers/smithers/ml/VOCdevkit/VOC2012\n",
      "\n",
      "There are 16551 training images containing a total of 49653 objects.        Files have been saved to /u/s/szanin/Smithers/smithers/ml/tutorials.\n",
      "\n",
      "There are 4952 test images containing a total of 14856 objects.        Files have been saved to    /u/s/szanin/Smithers/smithers/ml/tutorials.\n"
     ]
    }
   ],
   "source": [
    "#%run ../dataset/create_json.py path_to_VOC2007dir path_to_VOC2012dir path_to_outputfolder\n",
    "%run ../dataset/create_json.py ../VOCdevkit/VOC2007/ ../VOCdevkit/VOC2012/ ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a PascalVOCDataset class\n",
    "\n",
    "In order to use the constructed dataset properly, we need to define a subclass of PyTorch Dataset, called PascaVOCDataset. For more details about the implementation see pascalvoc_dataset.py in the dataset folder.\n",
    "\n",
    "\n",
    "The PascalVOCdataset class has been defined to detect your training and test datasets from the JSON files created above. It needs a __len__ method defined, which returns the size of the dataset, and a __getitem__ method which returns the i-th image, bounding boxes of the objects in this image, and labels for the objects in this image, using the JSON files we saved earlier.\n",
    "\n",
    "You will notice that it also returns the perceived detection difficulties of each of these objects, but these are not actually used in training the model. They are required only in the Evaluation stage for computing the Mean Average Precision (mAP) metric. We also have the option of filtering out difficult objects entirely from our data to speed up training at the cost of some accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from smithers.ml.dataset.pascalvoc_dataset import PascalVOCDataset\n",
    "\n",
    "keep_difficult = True\n",
    "\n",
    "# data_folder corresponds to the output folder defined before, where the JSON files have been saved\n",
    "data_folder = './'\n",
    "#data_folder = '/u/s/szanin/Smithers/smithers/ml/tutorials/'\n",
    "# Load train data\n",
    "train_dataset = PascalVOCDataset(data_folder,\n",
    "                                 split='train',\n",
    "                                 keep_difficult=keep_difficult)\n",
    "\n",
    "# Load test data\n",
    "test_dataset = PascalVOCDataset(data_folder,\n",
    "                                split='test',\n",
    "                                keep_difficult=keep_difficult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract smaller datasets\n",
    "If you want to test your model against a smaller dataset than PascalVOC, you can exctract a set of images from the original PascalVOC using sample_dataset.py in the dataset folder.\n",
    "\n",
    "You can thus extract a dataset composed of N images divided in M classes, where N and M are less than the total number of images and classes composing the dataset under consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../dataset/sample_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous line (refer to sample_dataset.py) we took cat and dogs as the only classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to split the subdataset in the train dataset (80% of the total) and the test dataset (the remaining 20%). To do so we use the same procedure found in the splitting section of customdata_objdec.ipynb tutorial.\n",
    "\n",
    "We first create the directories and files needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "(   \n",
    "    cd /u/s/szanin/Smithers/smithers/ml/tutorials/VOC_dog_cat/;\n",
    "    touch datafile.txt;\n",
    "    mkdir ImageSets;\n",
    "    cd ImageSets;\n",
    "    mkdir Main;\n",
    "    cd Main;\n",
    "    touch trainval.txt;\n",
    "    touch test.txt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_p = [0,1,2,3]\n",
    "len_list = len(list_p)\n",
    "list_p[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "with open('/u/s/szanin/Smithers/smithers/ml/tutorials/VOC_dog_cat/datafile.txt', 'w') as datafile:\n",
    "    dir_list = os.listdir('/u/s/szanin/Smithers/smithers/ml/tutorials/VOC_dog_cat/JPEGImages')\n",
    "    num_files = len(dir_list)\n",
    "    for element in dir_list[:-1]:\n",
    "        datafile.write('{}\\n'.format(element.removesuffix('.jpg')))\n",
    "    datafile.write('{}'.format(dir_list[-1].removesuffix('.jpg'))) # the last element added does not need the new line characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'003127'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/u/s/szanin/Smithers/smithers/ml/tutorials/VOC_dog_cat/datafile.txt','r') as f:\n",
    "#with open(\"../data_lab/datafile.txt\", \"r\") as f:\n",
    " # in Windows you may need to put rb instead of r mode \n",
    "   data = f.read().split('\\n')\n",
    "data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_file = '/u/s/szanin/Smithers/smithers/ml/tutorials/VOC_dog_cat/ImageSets/Main/trainval.txt'\n",
    "test_file = '/u/s/szanin/Smithers/smithers/ml/tutorials/VOC_dog_cat/ImageSets/Main/test.txt'\n",
    "#test_file = '../data_lab/VOC2007/ImageSets/Main/test.txt'\n",
    "\n",
    "with open('/u/s/szanin/Smithers/smithers/ml/tutorials/VOC_dog_cat/datafile.txt','r') as f:\n",
    "#with open(\"../data_lab/datafile.txt\", \"r\") as f:\n",
    " # in Windows you may need to put rb instead of r mode \n",
    "   data = f.read().split('\\n')\n",
    "   data = numpy.array(data)  #convert array to numpy type array\n",
    "\n",
    "   train ,test = train_test_split(data,test_size=0.2)      \n",
    "   split = [train, test] \n",
    "   # the ouputs here are two lists containing train-test split of inputs.\n",
    "   lengths = [len(train), len(test)]\n",
    "   out_train = open(train_file,\"w\")\n",
    "   out_test = open(test_file, \"w\")\n",
    "   out_file = [out_train, out_test]\n",
    "   out = 0\n",
    "   for l in lengths:\n",
    "        for i in range(l):\n",
    "            name_img = split[out][i]\n",
    "            out_file[out].write(name_img + '\\n')\n",
    "        out_file[out].close()    \n",
    "        out += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/u/s/szanin/Smithers/smithers/ml/tutorials/VOC_dog_cat\n",
      "/u/s/szanin/Smithers/smithers/ml/tutorials/VOC_dog_cat\n",
      "/u/s/szanin/Smithers/smithers/ml/tutorials/None\n",
      "\n",
      "There are 240 training images containing a total of 720 objects.        Files have been saved to /u/s/szanin/Smithers/smithers/ml/tutorials.\n",
      "\n",
      "There are 60 test images containing a total of 180 objects.        Files have been saved to    /u/s/szanin/Smithers/smithers/ml/tutorials.\n"
     ]
    }
   ],
   "source": [
    "%run ../dataset/create_json.py ./VOC_dog_cat/ None ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('reducedcnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c5bf16c94eb6f9341fa612a12f652937166e39821fa969ec7095b77ab48ffd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
