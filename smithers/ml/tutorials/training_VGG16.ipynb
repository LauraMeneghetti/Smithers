{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training VGG16 - Tutorial\n",
    "\n",
    "In this tutorial we will present how to train a VGG16 network and create a checkpoint for the trained network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Firstly, we start by importing all the necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "\n",
    "from smithers.ml.vgg import VGG\n",
    "from smithers.ml.utils import get_seq_model, save_checkpoint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the proper device\n",
    "The following lines will detect if a gpu is available in the system running this tutorial. If that is the case, all the objects of the following tutorial will be allocated in the gpu, thus speeding up the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda has been detected as the device which the script will be run on.\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, '../')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"{device} has been detected as the device which the script will be run on.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instatiation of the VGG16 network object, as defined in smithers.ml.vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded base model.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "VGGnet = VGG(    cfg=None,\n",
    "                 classifier='cifar',\n",
    "                 batch_norm=False,\n",
    "                 num_classes=10,\n",
    "                 init_weights=False,\n",
    "                 pretrain_weights=None).to(device)\n",
    "VGGnet.make_layers()\n",
    "VGGnet._initialize_weights()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(VGGnet.parameters(), lr=0.001, momentum=0.9)\n",
    "seq_model = get_seq_model(VGGnet).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of the CIFAR10 dataset\n",
    "As stated before, we use the CIFAR10 dataset (already implemented in PyTorch) to test our technique. It is a computer-vision dataset used for object recognition. It consists of 60000 32 Ã— 32 colour images divided in 10 non-overlapping classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.\n",
    "\n",
    "See https://www.cs.toronto.edu/~kriz/cifar.html for more details on this dataset and on how to download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../datasets/CIFAR10/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01073765754699707,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 170498071,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566733cdc7a3447786557b5b9eb8e028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../datasets/CIFAR10/cifar-10-python.tar.gz to ../datasets/CIFAR10/\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8 #this can be changed\n",
    "data_path = '../datasets/' \n",
    "# transform functions: take in input a PIL image and apply this\n",
    "# transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "train_dataset = datasets.CIFAR10(root=data_path + 'CIFAR10/',\n",
    "                                 train=True,\n",
    "                                 download=True,\n",
    "                                 transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_dataset = datasets.CIFAR10(root=data_path + 'CIFAR10/',\n",
    "                                train=False,\n",
    "                                download=True,\n",
    "                                transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "train_labels = torch.tensor(train_loader.dataset.targets).to(device)\n",
    "targets = list(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary accuracy test\n",
    "We will now compute the accuracy of the randomly initialized network on the test images before training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of network on test images is 13.89%\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "count = 0\n",
    "seq_model.eval()\n",
    "for test, y_test in iter(test_loader):\n",
    "    with torch.no_grad():\n",
    "        output = seq_model(test.to(device)).to(device)\n",
    "        ps = torch.exp(output)\n",
    "        _, predicted = torch.max(output.data,1)\n",
    "        total += y_test.size(0)\n",
    "        correct += (predicted == y_test.to(device)).sum().item()\n",
    "        count += 1\n",
    "print('Accuracy of network on test images is {:.2f}%'.format(100*correct/total), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training phase \n",
    "The following lines of code will train the network on the train images from the CIFAR10 dataset. Beware that training time can be very long and a gpu is recommended.\n",
    "\n",
    "It is possible to change the number of epochs for the training: if a large number is given, the network will perform better on the train images but will take longer to train, vice versa for a low number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 1 is now starting.\n",
      "Epoch number 1 is now completed.\n",
      "Epoch number 2 is now starting.\n",
      "Epoch number 2 is now completed.\n",
      "Epoch number 3 is now starting.\n",
      "Epoch number 3 is now completed.\n",
      "Epoch number 4 is now starting.\n",
      "Epoch number 4 is now completed.\n",
      "Epoch number 5 is now starting.\n",
      "Epoch number 5 is now completed.\n",
      "Epoch number 6 is now starting.\n",
      "Epoch number 6 is now completed.\n",
      "Epoch number 7 is now starting.\n",
      "Epoch number 7 is now completed.\n",
      "Epoch number 8 is now starting.\n",
      "Epoch number 8 is now completed.\n",
      "Epoch number 9 is now starting.\n",
      "Epoch number 9 is now completed.\n",
      "Epoch number 10 is now starting.\n",
      "Epoch number 10 is now completed.\n",
      "The network has been successfully trained for 10 epochs.\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch number {epoch+1} is now starting.\", flush=True)\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = VGGnet(inputs)\n",
    "        outputs = outputs[1]\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch number {epoch+1} is now completed.\", flush=True)\n",
    "print(f\"The network has been successfully trained for {n_epochs} epochs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy after the training phase\n",
    "We can check whether the accuracy on the test images has improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of network on test images is 80.93%\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "count = 0\n",
    "seq_model.eval()\n",
    "for test, y_test in iter(test_loader):\n",
    "    with torch.no_grad():\n",
    "        output = seq_model(test.to(device)).to(device)\n",
    "        ps = torch.exp(output)\n",
    "        _, predicted = torch.max(output.data,1)\n",
    "        total += y_test.size(0)\n",
    "        correct += (predicted == y_test.to(device)).sum().item()\n",
    "        count += 1\n",
    "print('Accuracy of network on test images is {:.2f}%'.format(100*correct/total), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a checkpoint of the state of the network\n",
    "Once the training is done, it would be nice to save the state of the network for a later use. To do so, we will use the save_checkpoint function defined in smither.ml.utils, that has been initially imported in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = # write here the desired path for the checkpoint\n",
    "save_checkpoint(n_epochs, VGGnet, path, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('reducedcnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c5bf16c94eb6f9341fa612a12f652937166e39821fa969ec7095b77ab48ffd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
