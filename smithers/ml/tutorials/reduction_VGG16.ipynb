{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction of VGG16\n",
    "In this tutorial we will present how to create a reduced version of VGG16 using the techniques described in the article ''A Dimensionality Reduction Approach for Convolutional Neural Networks'', Meneghetti L., Demo N., Rozza G., https://arxiv.org/abs/2110.09163 (2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "We start by importing all the necessary libraries and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "\n",
    "from smithers.ml.vgg import VGG\n",
    "from smithers.ml.utils import load_checkpoint, save_checkpoint, get_seq_model, Total_param, Total_flops, compute_loss, train_kd\n",
    "from smithers.ml.netadapter import NetAdapter\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the proper device\n",
    "The following lines will detect if a gpu is available in the system running this tutorial. If that is the case, all the objects of the following tutorial will be allocated in the gpu, thus speeding up the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda has been detected as the device which the script will be run on.\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, '../')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"{device} has been detected as the device which the script will be run on.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16 initialization\n",
    "Instatiation of the VGG16 network object, as defined in smithers.ml.vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded base model.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "VGGnet = VGG(    cfg=None,\n",
    "                 classifier='cifar',\n",
    "                 batch_norm=False,\n",
    "                 num_classes=10,\n",
    "                 init_weights=False,\n",
    "                 pretrain_weights=None).to(device)\n",
    "VGGnet.make_layers()\n",
    "VGGnet._initialize_weights()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(VGGnet.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of the dataset\n",
    "### CIFAR10 Dataset\n",
    "As stated before, we use the CIFAR10 dataset (already implemented in PyTorch) to test our technique. It is a computer-vision dataset used for object recognition. It consists of 60000 32 Ã— 32 colour images divided in 10 non-overlapping classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.\n",
    "\n",
    "See https://www.cs.toronto.edu/~kriz/cifar.html for more details on this dataset and on how to download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8 #this can be changed\n",
    "data_path = '../datasets/' \n",
    "# transform functions: take in input a PIL image and apply this\n",
    "# transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "train_dataset = datasets.CIFAR10(root=data_path + 'CIFAR10/',\n",
    "                                 train=True,\n",
    "                                 download=True,\n",
    "                                 transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_dataset = datasets.CIFAR10(root=data_path + 'CIFAR10/',\n",
    "                                train=False,\n",
    "                                download=True,\n",
    "                                transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "train_labels = torch.tensor(train_loader.dataset.targets)\n",
    "targets = list(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of the model\n",
    "First of all we need to load the model we want to use (in this case VGG16) starting from a checkpoint file, i.e. a file containing the status of the model after a training process with a chosen dataset. Here we will use the CIFAR10 dataset, but we will also show how to generalize everythong using a custom dataset.\n",
    "\n",
    "It is important to highlight that the models of VGG-nets implemented in PyTorch (https://pytorch.org/hub/pytorch_vision_vgg/), e.g. \n",
    "```\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True),\n",
    "```\n",
    "\n",
    "are models pre-trained on the ImageNet dataset, that consists of images of dimensions 224x224. Therefore, in order to use datasets like the CIFAR10, composed of images 32x32, we need to change the architecture of VGG-nets, as was done in the file 'vgg.py'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../')\n",
    "pretrained = '/u/s/szanin/Smithers/smithers/ml/tutorials/check_vgg_cifar10_60_v2.pth.tar'\n",
    "# pretrained = insert here the proper path for your device\n",
    "model = VGGnet.to(device)\n",
    "load_checkpoint(model, pretrained)\n",
    "seq_model = get_seq_model(model).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training phase\n",
    "For further details on this phase, see the relative tutorial 'training_VGG16.ipynb'.\n",
    "(skip this cell if a trained VGGnet has been loaded in the previous cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.107\n",
      "[1,  4000] loss: 1.783\n",
      "[1,  6000] loss: 1.605\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(VGGnet.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = VGGnet(inputs)\n",
    "        outputs = outputs[1]\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset\n",
    "If we want to use a custom dataset, we need firstly to construct it, following for example the tutorial on the construction of a custom dataset for the problem of Image Recognition. Hence, the previuous cell will be substitute with the following one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from collections import OrderedDict\n",
    "from smithers.ml.imagerec_dataset import Imagerec_Dataset\n",
    "\n",
    "# load custom dataset for training and testing\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "data = pd.read_csv('../dataset_imagerec/dataframe.csv')\n",
    "data_path = '../dataset_imagerec/'\n",
    "# SPLIT OF THE DATASET\n",
    "batch_size = 128\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed = 42\n",
    "\n",
    "dataset_size = len(data)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "print('train data', len(train_indices))\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "resize_dim = [32, 32]\n",
    "\n",
    "dataset_imagerec = Imagerec_Dataset(data, data_path, resize_dim, transform)\n",
    "train_dataset = dataset_imagerec.getdata(train_indices)\n",
    "train_loader = torch.utils.data.DataLoader(dataset_imagerec,\n",
    "                                           batch_size=batch_size,\n",
    "                                           sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_imagerec,\n",
    "                                          batch_size=batch_size,\n",
    "                                          sampler=valid_sampler)\n",
    "\n",
    "classes = ('Cups', 'Dishes', 'Glass', 'Mixed')\n",
    "#classes = ('class_1', 'class_2', 'class_3', 'class_4')\n",
    "n_class = len(classes)\n",
    "targets = list(dataset_imagerec.targets[train_indices])\n",
    "train_labels = torch.tensor(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction of VGG16\n",
    "We now perform the reduction of VGG16 using the module NetAdapter. In this case we use 5 as cut off index and 50 as dimension of the reduced space. For the reduced method and the input-output mapping there are multiple choices: 'POD', 'AS', 'RandSVD' or 'HOSVD' for the first one, and 'PCE' or 'FNN' for the latter.\n",
    "\n",
    "Let's start by computing the current accuracy of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of network on test images is 89.0500....count: 250\n",
      "Accuracy of network on test images is 89.0250....count: 500\n",
      "Accuracy of network on test images is 88.8167....count: 750\n",
      "Accuracy of network on test images is 88.7750....count: 1000\n",
      "Accuracy of network on test images is 88.8200....count: 1250\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "count = 0\n",
    "seq_model.eval()\n",
    "for test, y_test in iter(test_loader):\n",
    "#Calculate the class probabilities (softmax) for img\n",
    "    with torch.no_grad():\n",
    "        output = seq_model(test.to(device)).to(device)\n",
    "        ps = torch.exp(output)\n",
    "        _, predicted = torch.max(output.data,1)\n",
    "        total += y_test.size(0)\n",
    "        correct += (predicted == y_test.to(device)).sum().item()\n",
    "        count += 1\n",
    "        if count % 250 == 0:\n",
    "            print(\"Accuracy of network on test images is {:.4f}....count: {}\".format(100*correct/total,  count ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells have be designed to illustrate the different approaches that one can follow in the reduction process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POD + FNN\n",
    "To get more details on this, please refer to fnn.py and the torch.svd function. General informations can be found in netadapter.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing reduction. Chosen reduction method is: POD\n",
      "Initializing dataset forwarding\n",
      "Dataset forwarding complete\n",
      "FNN training initialized\n",
      "FNN training completed\n",
      "RedNet(\n",
      "  (premodel): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  )\n",
      "  (proj_model): Linear(in_features=4096, out_features=50, bias=False)\n",
      "  (inout_map): FNN(\n",
      "    (model): Sequential(\n",
      "      (0): Linear(in_features=50, out_features=20, bias=True)\n",
      "      (1): Softplus(beta=1, threshold=20)\n",
      "      (2): Linear(in_features=20, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cutoff_idx = 7 \n",
    "red_dim = 50 \n",
    "red_method = 'POD' \n",
    "inout_method = 'FNN'\n",
    "n_class = 10\n",
    "netadapter = NetAdapter(cutoff_idx, red_dim, red_method, inout_method)\n",
    "red_model = netadapter.reduce_net(seq_model, train_dataset, train_labels, train_loader, n_class)\n",
    "print(red_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandSVD + FNN\n",
    "To get more details on this, please refer to fnn.py and the torch.svd function. General informations can be found in netadapter.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing reduction. Chosen reduction method is: RandSVD\n",
      "Initializing dataset forwarding\n",
      "Dataset forwarding complete\n",
      "FNN training initialized\n",
      "FNN training completed\n",
      "RedNet(\n",
      "  (premodel): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  )\n",
      "  (proj_model): Linear(in_features=4096, out_features=50, bias=False)\n",
      "  (inout_map): FNN(\n",
      "    (model): Sequential(\n",
      "      (0): Linear(in_features=50, out_features=20, bias=True)\n",
      "      (1): Softplus(beta=1, threshold=20)\n",
      "      (2): Linear(in_features=20, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cutoff_idx = 7 \n",
    "red_dim = 50 \n",
    "red_method = 'RandSVD' \n",
    "inout_method = 'FNN'\n",
    "n_class = 10\n",
    "netadapter = NetAdapter(cutoff_idx, red_dim, red_method, inout_method)\n",
    "red_model = netadapter.reduce_net(seq_model, train_dataset, train_labels, train_loader, n_class)\n",
    "print(red_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AHOSVD + FNN\n",
    "To get more details on this, please refer to fnn.py, AHOSVD.py, hosvd.py and tensor_product_layer.py. General informations can be found in netadapter.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing reduction. Chosen reduction method is: HOSVD\n",
      "Initializing dataset forwarding\n",
      "Dataset forwarding complete\n",
      "FNN training initialized\n",
      "FNN training completed\n",
      "RedNet(\n",
      "  (premodel): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  )\n",
      "  (proj_model): tensor_product_layer(in_dimensions=[256, 4, 4], out_dimensions=[35, 3, 3])\n",
      "  (inout_map): FNN(\n",
      "    (model): Sequential(\n",
      "      (0): Linear(in_features=315, out_features=20, bias=True)\n",
      "      (1): Softplus(beta=1, threshold=20)\n",
      "      (2): Linear(in_features=20, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cutoff_idx = 7\n",
    "mode_list_batch=[25, 35, 3, 3]\n",
    "red_method= 'HOSVD'\n",
    "red_dim = 3 * 3 * 35    # red_dim, in the case of HOSVD, must be the product of all but the first enteries of mode_list_batch\n",
    "inout_method = 'FNN'\n",
    "n_class = 10  \n",
    "\n",
    "netadapter = NetAdapter(cutoff_idx, red_dim, red_method, inout_method)\n",
    "red_model = netadapter.reduce_net(seq_model, train_dataset, train_labels, train_loader, n_class, device = device, mode_list_batch = mode_list_batch).to(device) \n",
    "print(red_model, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the reduced network\n",
    "Now that the reduced network has been defined, we can train it. The technique used is referred to as \"knowledge distillation\". In it, two model are used: the trained model, called student model, and an already trained module, the teacher model, which the student model will learn from. In our case, the full and trained VGG16 network will act as teacher model.\n",
    "\n",
    "At the end of the training the model will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss -6.695368256926536e-06\n",
      " Top 1:  Accuracy: 5970.0/50000 (11.94%)\n",
      "Test Loss: -0.3347684128463268\n",
      "Test Loss -3.5530161729455e-05\n",
      " Top 1:  Accuracy: 1137.0/10000 (11.37%)\n",
      "Test Loss: -0.35530161729454995\n",
      "EPOCH 1\n",
      "Train Loss kd: 7.700555324554444e-06\n",
      "Test Loss -0.001128806265487671\n",
      " Top 1:  Accuracy: 8473.0/10000 (84.73%)\n",
      "Test Loss: -11.28806265487671\n",
      "EPOCH 2\n",
      "Train Loss kd: 5.962002873420716e-06\n",
      "Test Loss -0.001205775994644165\n",
      " Top 1:  Accuracy: 8714.0/10000 (87.14%)\n",
      "Test Loss: -12.057759946441651\n",
      "EPOCH 3\n",
      "Train Loss kd: 1.0420675575733186e-06\n",
      "Test Loss -0.0014075436742401123\n",
      " Top 1:  Accuracy: 8774.0/10000 (87.74%)\n",
      "Test Loss: -14.075436742401124\n",
      "EPOCH 4\n",
      "Train Loss kd: 2.5630873441696166e-06\n",
      "Test Loss -0.0014386886726379395\n",
      " Top 1:  Accuracy: 8793.0/10000 (87.93%)\n",
      "Test Loss: -14.386886726379394\n",
      "EPOCH 5\n",
      "Train Loss kd: 7.660793513059617e-07\n",
      "Test Loss -0.0015669711363220215\n",
      " Top 1:  Accuracy: 8833.0/10000 (88.33%)\n",
      "Test Loss: -15.669711363220214\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam([{\n",
    "            'params': red_model.premodel.parameters(),\n",
    "            'lr': 1e-4\n",
    "            }, {\n",
    "            'params': red_model.proj_model.parameters(),\n",
    "            'lr': 1e-5\n",
    "            }, {\n",
    "            'params': red_model.inout_map.parameters(),\n",
    "            'lr': 1e-5\n",
    "            }])\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_loss.append(compute_loss(red_model, device, train_loader))\n",
    "test_loss.append(compute_loss(red_model, device, test_loader))\n",
    "\n",
    "        \n",
    "epochs = 5\n",
    "filename = './cifar10_VGG16_RedNet'+red_method+'_cutIDx_%d.pth'%(cutoff_idx)\n",
    "for epoch in range(1, epochs + 1):                     \n",
    "    print('EPOCH {}'.format(epoch), flush=True)\n",
    "    train_loss.append(\n",
    "            train_kd(red_model,\n",
    "            model,\n",
    "            device,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            train_max_batch=200,\n",
    "            alpha=0.1,\n",
    "            temperature=1.,\n",
    "            epoch=epoch))\n",
    "    test_loss.append(compute_loss(red_model, device, test_loader))\n",
    "#torch.save([red_model.state_dict(), train_loss, test_loss], filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a reduced network checkpoint\n",
    "If a reduced network has been already defined and saved on the computer, it can be loaded with the following instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RedNet checkpoint (trained for 5 epoches) was correctly loaded.\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(filename):\n",
    "    [rednet_pretrained, train_loss,test_loss] = torch.load(filename)\n",
    "    red_model.load_state_dict(rednet_pretrained)\n",
    "    print('RedNet checkpoint (trained for {} epoches) was correctly loaded.'.format(epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy testing\n",
    "We can further test the accuracy of the network with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of network on test images is 87.1000....count: 250\n",
      "Accuracy of network on test images is 87.7750....count: 500\n",
      "Accuracy of network on test images is 88.0833....count: 750\n",
      "Accuracy of network on test images is 88.1625....count: 1000\n",
      "Accuracy of network on test images is 88.3300....count: 1250\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "count = 0\n",
    "for test, y_test in iter(test_loader):\n",
    "    with torch.no_grad():\n",
    "        output = red_model(test.to(device))\n",
    "        ps = torch.exp(output)\n",
    "        _, predicted = torch.max(output.data,1)\n",
    "        total += y_test.size(0)\n",
    "        correct += (predicted == y_test.to(device)).to(device).sum().item()\n",
    "        count += 1\n",
    "        if count%250 == 0:\n",
    "            print(\"Accuracy of network on test images is {:.4f}....count: {}\".format(100*correct/total,  count ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storage and flops needed for the model\n",
    "The following lines of code provide the amounts of storage needed to save the reduced model together with the number of floating point operations needed to compute them.\n",
    "\n",
    "We start by counting the number of non-zero entries (nnz) of the three components of the reduced network (this method only concerns the POD+FNN and RandSVD+FNN techniques, regarding the storage) and the flops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre nnz = 6.62, proj_model nnz=0.03, FNN nnz=0.0249\n",
      "flops:  Pre = 190.51, proj_model = 0.00, FNN =0.01\n"
     ]
    }
   ],
   "source": [
    "rednet_storage = torch.zeros(3)\n",
    "rednet_flops = torch.zeros(3)\n",
    "\n",
    "rednet_storage[0], rednet_storage[1], rednet_storage[2] = [\n",
    "    Total_param(red_model.premodel),\n",
    "    Total_param(red_model.proj_model),\n",
    "    Total_param(red_model.inout_map)]\n",
    "\n",
    "rednet_flops[0], rednet_flops[1], rednet_flops[2] = [\n",
    "    Total_flops(red_model.premodel, device),\n",
    "    Total_flops(red_model.proj_model, device),\n",
    "    Total_flops(red_model.inout_map, device)]\n",
    "\n",
    "\n",
    "print('Pre nnz = {:.2f}, proj_model nnz={:.2f}, FNN nnz={:.4f}'.format(\n",
    "      rednet_storage[0], rednet_storage[1],\n",
    "      rednet_storage[2]))\n",
    "print('flops:  Pre = {:.2f}, proj_model = {:.2f}, FNN ={:.2f}'.format(\n",
    "       rednet_flops[0], rednet_flops[1], rednet_flops[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define another method that counts the storage needed for saving the reduced model (in MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the storage needed by the RedNet model.\n",
      "Components summary:\n",
      "premodel.0.weight \t torch.Size([64, 3, 3, 3])\n",
      "premodel.0.bias \t torch.Size([64])\n",
      "premodel.2.weight \t torch.Size([64, 64, 3, 3])\n",
      "premodel.2.bias \t torch.Size([64])\n",
      "premodel.5.weight \t torch.Size([128, 64, 3, 3])\n",
      "premodel.5.bias \t torch.Size([128])\n",
      "premodel.7.weight \t torch.Size([128, 128, 3, 3])\n",
      "premodel.7.bias \t torch.Size([128])\n",
      "premodel.10.weight \t torch.Size([256, 128, 3, 3])\n",
      "premodel.10.bias \t torch.Size([256])\n",
      "premodel.12.weight \t torch.Size([256, 256, 3, 3])\n",
      "premodel.12.bias \t torch.Size([256])\n",
      "premodel.14.weight \t torch.Size([256, 256, 3, 3])\n",
      "premodel.14.bias \t torch.Size([256])\n",
      "proj_model.param0 \t torch.Size([35, 256])\n",
      "proj_model.param1 \t torch.Size([3, 4])\n",
      "proj_model.param2 \t torch.Size([3, 4])\n",
      "inout_map.model.0.weight \t torch.Size([20, 315])\n",
      "inout_map.model.0.bias \t torch.Size([20])\n",
      "inout_map.model.2.weight \t torch.Size([10, 20])\n",
      "inout_map.model.2.bias \t torch.Size([10])\n",
      "\n",
      "\n",
      "The used MB are: 7.004007816314697\n"
     ]
    }
   ],
   "source": [
    "print('Computing the storage needed by the RedNet model.\\nComponents summary:')\n",
    "storage = 0\n",
    "for param_tensor in red_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", red_model.state_dict()[param_tensor].size())\n",
    "    storage += torch.prod(torch.tensor(list(red_model.state_dict()[param_tensor].size())))\n",
    "print(f\"\\n\\nThe MB used are: {4 * storage / 10 ** 6}\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c5bf16c94eb6f9341fa612a12f652937166e39821fa969ec7095b77ab48ffd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
