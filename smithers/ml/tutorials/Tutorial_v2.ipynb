{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 reduction tutorial\n",
    "\n",
    "In this tutorial we will present how to create a reduced version of VGG16 using the techniques described in the article ''A Dimensionality Reduction Approach for Convolutional Neural Networks'', Meneghetti L., Demo N., Rozza G., https://arxiv.org/abs/2110.09163 (2021)\n",
    "\n",
    "\n",
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "\n",
    "from smithers.ml.vgg import VGG\n",
    "from smithers.ml.utils import get_seq_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from smithers.ml.utils import randomized_range_finder\n",
    "from smithers.ml.utils import randomized_svd\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETTING PROPER DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG INSTANTIATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded base model.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "VGGnet = VGG(    cfg=None,\n",
    "                 classifier='cifar',\n",
    "                 batch_norm=False,\n",
    "                 num_classes=10,\n",
    "                 init_weights=False,\n",
    "                 pretrain_weights=None)\n",
    "VGGnet = VGGnet.to(device) #MODIF\n",
    "VGGnet.make_layers()\n",
    "VGGnet._initialize_weights()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(VGGnet.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT & EXPORT FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint_torch(epoch, model, path, optimizer):\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "            }, path)\n",
    "\n",
    "\n",
    "def load_checkpoint(model, checkpoint_path):\n",
    "    model.load_state_dict(torch.load(checkpoint_path)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10 LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8 #this can be changed\n",
    "data_path = '../datasets/' \n",
    "# transform functions: take in input a PIL image and apply this\n",
    "# transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "train_dataset = datasets.CIFAR10(root=data_path + 'CIFAR10/',\n",
    "                                 train=True,\n",
    "                                 download=True,\n",
    "                                 transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_dataset = datasets.CIFAR10(root=data_path + 'CIFAR10/',\n",
    "                                train=False,\n",
    "                                download=True,\n",
    "                                transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "train_labels = torch.tensor(train_loader.dataset.targets).to(device)\n",
    "targets = list(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING (UNCOMMENT IF NO CHECKPOINT AVAILABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' print(\\'inizio training\\', flush=True)\\nprint(\\'Training iniziato\\') #MODIF\\nfor epoch in range(60):  # loop over the dataset multiple times\\n    print(\"Inizia ora l\\'epoca \"+str(epoch), flush=True)\\n    running_loss = 0.0\\n    for i, data in enumerate(train_loader, 0):\\n        # get the inputs; data is a list of [inputs, labels]\\n        inputs, labels = data\\n        inputs = inputs.to(device)\\n        labels = labels.to(device)\\n\\n\\n        # zero the parameter gradients\\n        optimizer.zero_grad()\\n\\n        # forward + backward + optimize\\n        outputs = VGGnet(inputs)\\n        outputs = outputs[1]\\n        loss = criterion(outputs, labels)\\n        loss.backward()\\n        optimizer.step()\\n\\n        # print statistics\\n        running_loss += loss.item()\\n        #if i % 2000 == 1999:    # print every 2000 mini-batches\\n        if i % 200 == 199:    # print every 200 mini-batches #MODIF\\n            print(\\'[%d, %5d] loss: %.3f\\' %\\n                  (epoch + 1, i + 1, running_loss / 2000), flush=True)\\n            running_loss = 0.0\\n\\n\\n\\n\\nsave_checkpoint_torch(60, VGGnet, \\'/u/s/szanin/Smithers/smithers/ml/tutorials/check_vgg_cifar10_60_stefano.pth.tar\\', optimizer)\\n '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" print('inizio training', flush=True)\n",
    "print('Training iniziato') #MODIF\n",
    "for epoch in range(60):  # loop over the dataset multiple times\n",
    "    print(\"Inizia ora l'epoca \"+str(epoch), flush=True)\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = VGGnet(inputs)\n",
    "        outputs = outputs[1]\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        #if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "        if i % 200 == 199:    # print every 200 mini-batches #MODIF\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000), flush=True)\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "save_checkpoint_torch(60, VGGnet, '/u/s/szanin/Smithers/smithers/ml/tutorials/check_vgg_cifar10_60_stefano.pth.tar', optimizer)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING A CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = '/u/s/szanin/Smithers/smithers/ml/tutorials/check_vgg_cifar10_60_v2.pth.tar' #Stefano's\n",
    "model = VGGnet\n",
    "load_checkpoint(model, pretrained)\n",
    "seq_model = get_seq_model(model)\n",
    "model = model.to(device) #MODIF\n",
    "seq_model = seq_model.to(device) #MODIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACCURACY OF THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of network on test images is 88.8200\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "count = 0\n",
    "VGGnet.eval()\n",
    "for test, y_test in iter(test_loader):\n",
    "    with torch.no_grad():\n",
    "        output = seq_model(test.to(device))\n",
    "        ps = torch.exp(output)\n",
    "        _, predicted = torch.max(output.data,1)\n",
    "        total += y_test.size(0)\n",
    "        correct += (predicted == y_test.to(device)).sum().item() #MODIF\n",
    "        count += 1\n",
    "print('Accuracy of network on test images is {:.4f}'.format(100*correct/total), flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REDUCTION OF THE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of network on test images is 89.4167....count: 300\n",
      "Accuracy of network on test images is 89.0833....count: 600\n",
      "Accuracy of network on test images is 89.1389....count: 900\n",
      "Accuracy of network on test images is 88.9271....count: 1200\n",
      "Initializing reduction. Chosen reduction method is: POD\n",
      "Siamo alla batch batch 0\n",
      "Siamo alla batch batch 1000\n",
      "Siamo alla batch batch 2000\n",
      "Siamo alla batch batch 3000\n",
      "Siamo alla batch batch 4000\n",
      "Siamo alla batch batch 5000\n",
      "Siamo alla batch batch 6000\n",
      "Siamo alla batch batch 0\n",
      "Siamo alla batch batch 1000\n",
      "Siamo alla batch batch 2000\n",
      "Siamo alla batch batch 3000\n",
      "Siamo alla batch batch 4000\n",
      "Siamo alla batch batch 5000\n",
      "Siamo alla batch batch 6000\n",
      "Le dimensioni delle due matrici sono: proj_mat = torch.Size([4096, 50]) e matrix (input di projection) = torch.Size([50000, 4096])\n",
      "si dovranno moltiplicare alcune righe di input_matrix per proj_matrix\n",
      "proj_mat è salvata su 0 (-1 = cpu, 0 = gpu)\n",
      "matrix è salvata su 0 (-1 = cpu, 0 = gpu)\n",
      "Comincia ora il training della FNN\n",
      "È terminato il training della rete neurale\n",
      "RedNet(\n",
      "  (premodel): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  )\n",
      "  (proj_model): Linear(in_features=4096, out_features=50, bias=False)\n",
      "  (inout_map): FNN(\n",
      "    (model): Sequential(\n",
      "      (0): Linear(in_features=50, out_features=20, bias=True)\n",
      "      (1): Softplus(beta=1, threshold=20)\n",
      "      (2): Linear(in_features=20, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "count = 0\n",
    "seq_model.eval()\n",
    "for test, y_test in iter(test_loader):\n",
    "#Calculate the class probabilities (softmax) for img\n",
    "    with torch.no_grad():\n",
    "        output = seq_model(test.to(device)) #MODIF\n",
    "        ps = torch.exp(output)\n",
    "        _, predicted = torch.max(output.data,1)\n",
    "        total += y_test.size(0)\n",
    "        correct += (predicted == y_test.to(device)).sum().item() #MODIF\n",
    "        count += 1\n",
    "        #print(\"Accuracy of network on test images is {:.4f}....count: {}\".format(100*correct/total,  count ))\n",
    "        if count%300 == 0:\n",
    "            print(\"Accuracy of network on test images is {:.4f}....count: {}\".format(100*correct/total,  count), flush=True)\n",
    "\n",
    "\n",
    "from smithers.ml.netadapter import NetAdapter\n",
    "\n",
    "cutoff_idx = 7\n",
    "red_dim = 50 \n",
    "#red_method = 'POD' \n",
    "red_method = 'POD'\n",
    "inout_method = 'FNN'\n",
    "n_class = 10 #MODIF\n",
    "\n",
    "netadapter = NetAdapter(cutoff_idx, red_dim, red_method, inout_method)\n",
    "red_model = netadapter.reduce_net(seq_model, train_dataset, train_labels, train_loader, n_class).to(device) #MODIF\n",
    "print(red_model, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RED MODEL STORAGE SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RED_MODEL's state_dict:\n",
      "premodel.0.weight \t torch.Size([64, 3, 3, 3])\n",
      "premodel.0.bias \t torch.Size([64])\n",
      "premodel.2.weight \t torch.Size([64, 64, 3, 3])\n",
      "premodel.2.bias \t torch.Size([64])\n",
      "premodel.5.weight \t torch.Size([128, 64, 3, 3])\n",
      "premodel.5.bias \t torch.Size([128])\n",
      "premodel.7.weight \t torch.Size([128, 128, 3, 3])\n",
      "premodel.7.bias \t torch.Size([128])\n",
      "premodel.10.weight \t torch.Size([256, 128, 3, 3])\n",
      "premodel.10.bias \t torch.Size([256])\n",
      "premodel.12.weight \t torch.Size([256, 256, 3, 3])\n",
      "premodel.12.bias \t torch.Size([256])\n",
      "premodel.14.weight \t torch.Size([256, 256, 3, 3])\n",
      "premodel.14.bias \t torch.Size([256])\n",
      "proj_model.weight \t torch.Size([50, 4096])\n",
      "inout_map.model.0.weight \t torch.Size([20, 50])\n",
      "inout_map.model.0.bias \t torch.Size([20])\n",
      "inout_map.model.2.weight \t torch.Size([10, 20])\n",
      "inout_map.model.2.bias \t torch.Size([10])\n",
      "The used MB are: 7.766071796417236\n"
     ]
    }
   ],
   "source": [
    "print(\"RED_MODEL's state_dict:\")\n",
    "storage = 0\n",
    "for param_tensor in red_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", red_model.state_dict()[param_tensor].size())\n",
    "    storage += torch.prod(torch.tensor(list(red_model.state_dict()[param_tensor].size())))\n",
    "print(f\"The used MB are: {4 * storage / 10 ** 6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FULL MODEL STORAGE SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL MODEL's state_dict:\n",
      "features.0.weight \t torch.Size([64, 3, 3, 3])\n",
      "features.0.bias \t torch.Size([64])\n",
      "features.2.weight \t torch.Size([64, 64, 3, 3])\n",
      "features.2.bias \t torch.Size([64])\n",
      "features.5.weight \t torch.Size([128, 64, 3, 3])\n",
      "features.5.bias \t torch.Size([128])\n",
      "features.7.weight \t torch.Size([128, 128, 3, 3])\n",
      "features.7.bias \t torch.Size([128])\n",
      "features.10.weight \t torch.Size([256, 128, 3, 3])\n",
      "features.10.bias \t torch.Size([256])\n",
      "features.12.weight \t torch.Size([256, 256, 3, 3])\n",
      "features.12.bias \t torch.Size([256])\n",
      "features.14.weight \t torch.Size([256, 256, 3, 3])\n",
      "features.14.bias \t torch.Size([256])\n",
      "features.17.weight \t torch.Size([512, 256, 3, 3])\n",
      "features.17.bias \t torch.Size([512])\n",
      "features.19.weight \t torch.Size([512, 512, 3, 3])\n",
      "features.19.bias \t torch.Size([512])\n",
      "features.21.weight \t torch.Size([512, 512, 3, 3])\n",
      "features.21.bias \t torch.Size([512])\n",
      "features.24.weight \t torch.Size([512, 512, 3, 3])\n",
      "features.24.bias \t torch.Size([512])\n",
      "features.26.weight \t torch.Size([512, 512, 3, 3])\n",
      "features.26.bias \t torch.Size([512])\n",
      "features.28.weight \t torch.Size([512, 512, 3, 3])\n",
      "features.28.bias \t torch.Size([512])\n",
      "classifier.0.weight \t torch.Size([10, 512])\n",
      "classifier.0.bias \t torch.Size([10])\n",
      "The used bytes are: 58.8792724609375\n"
     ]
    }
   ],
   "source": [
    "print(\"FULL MODEL's state_dict:\")\n",
    "storage = 0\n",
    "for param_tensor in VGGnet.state_dict():\n",
    "    print(param_tensor, \"\\t\", VGGnet.state_dict()[param_tensor].size())\n",
    "    storage += torch.prod(torch.tensor(list(VGGnet.state_dict()[param_tensor].size())))\n",
    "print(f\"The used bytes are: {4 * storage / 10 ** 6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_tensor in red_model.state_dict():\n",
    "    param_tensor_numpy = red_model.state_dict()[param_tensor].detach().to('cpu').numpy()\n",
    "    np.save('numpy_tensor_'+str(param_tensor), param_tensor_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STORAGE AND FLOPS COUNTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.6204e+00, 7.8125e-01, 4.6921e-03]) tensor([1.9051e+02, 2.0480e-01, 1.2000e-03])\n"
     ]
    }
   ],
   "source": [
    "from smithers.ml.utils import Total_param, Total_flops\n",
    "from smithers.ml.utils import compute_loss, train_kd\n",
    "\n",
    "\n",
    "rednet_storage = torch.zeros(3)\n",
    "rednet_flops = torch.zeros(3)\n",
    "\n",
    "rednet_storage[0], rednet_storage[1], rednet_storage[2] = [\n",
    "    Total_param(red_model.premodel),\n",
    "    Total_param(red_model.proj_model),\n",
    "    Total_param(red_model.inout_map)]\n",
    "\n",
    "rednet_flops[0], rednet_flops[1], rednet_flops[2] = [\n",
    "    Total_flops(red_model.premodel, device),\n",
    "    Total_flops(red_model.proj_model, device),\n",
    "    Total_flops(red_model.inout_map, device)]\n",
    "print(rednet_storage, rednet_flops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING OF THE REDUCED NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of network on test images is 7.5000....count: 50\n",
      "Accuracy of network on test images is 8.6250....count: 100\n",
      "Accuracy of network on test images is 8.8333....count: 150\n",
      "Accuracy of network on test images is 9.3750....count: 200\n",
      "Accuracy of network on test images is 9.1500....count: 250\n",
      "Accuracy of network on test images is 9.1250....count: 300\n",
      "Accuracy of network on test images is 8.9286....count: 350\n",
      "Accuracy of network on test images is 8.9375....count: 400\n",
      "Accuracy of network on test images is 9.0000....count: 450\n",
      "Accuracy of network on test images is 9.0750....count: 500\n",
      "Accuracy of network on test images is 9.0455....count: 550\n",
      "Accuracy of network on test images is 8.9583....count: 600\n",
      "Accuracy of network on test images is 8.9423....count: 650\n",
      "Accuracy of network on test images is 9.0000....count: 700\n",
      "Accuracy of network on test images is 8.8500....count: 750\n",
      "Accuracy of network on test images is 8.7812....count: 800\n",
      "Accuracy of network on test images is 8.8382....count: 850\n",
      "Accuracy of network on test images is 8.8472....count: 900\n",
      "Accuracy of network on test images is 8.8026....count: 950\n",
      "Accuracy of network on test images is 8.7625....count: 1000\n",
      "Accuracy of network on test images is 8.7024....count: 1050\n",
      "Accuracy of network on test images is 8.6364....count: 1100\n",
      "Accuracy of network on test images is 8.6522....count: 1150\n",
      "Accuracy of network on test images is 8.6771....count: 1200\n",
      "Accuracy of network on test images is 8.8000....count: 1250\n",
      "Pre nnz = 6.62, proj_model nnz=0.78, FNN nnz=0.0047\n",
      "flops:  Pre = 190.51, proj_model = 0.20, FNN =0.00\n",
      "Test Loss 1.2704684056277574e-05\n",
      " Top 1:  Accuracy: 4022.0/50000 (8.04%)\n",
      "Test Loss: 0.6352342028138787\n",
      "Test Loss 5.9109945001006124e-05\n",
      " Top 1:  Accuracy: 880.0/10000 (8.80%)\n",
      "Test Loss: 0.5910994500100613\n",
      "EPOCH 1\n",
      "Train Loss kd: 2.283362627029419e-05\n",
      "Test Loss -0.0012677085314178466\n",
      " Top 1:  Accuracy: 8467.0/10000 (84.67%)\n",
      "Test Loss: -12.677085314178466\n",
      "EPOCH 2\n",
      "Train Loss kd: 3.73248815536499e-06\n",
      "Test Loss -0.0017249752021026613\n",
      " Top 1:  Accuracy: 8722.0/10000 (87.22%)\n",
      "Test Loss: -17.249752021026612\n",
      "EPOCH 3\n",
      "Train Loss kd: 6.850305944681168e-07\n",
      "Test Loss -0.0018070751226806642\n",
      " Top 1:  Accuracy: 8798.0/10000 (87.98%)\n",
      "Test Loss: -18.070751226806642\n",
      "EPOCH 4\n",
      "Train Loss kd: 1.4445267617702485e-07\n",
      "Test Loss -0.001933090623321533\n",
      " Top 1:  Accuracy: 8814.0/10000 (88.14%)\n",
      "Test Loss: -19.33090623321533\n",
      "EPOCH 5\n",
      "Train Loss kd: 3.36933434009552e-06\n",
      "Test Loss -0.002076308215866089\n",
      " Top 1:  Accuracy: 8873.0/10000 (88.73%)\n",
      "Test Loss: -20.76308215866089\n",
      "EPOCH 6\n",
      "Train Loss kd: 1.0597562044858932e-06\n",
      "Test Loss -0.002157911942977905\n",
      " Top 1:  Accuracy: 8877.0/10000 (88.77%)\n",
      "Test Loss: -21.579119429779052\n",
      "EPOCH 7\n",
      "Train Loss kd: 1.357179582118988e-06\n",
      "Test Loss -0.0022729197283172606\n",
      " Top 1:  Accuracy: 8899.0/10000 (88.99%)\n",
      "Test Loss: -22.729197283172606\n",
      "EPOCH 8\n",
      "Train Loss kd: 4.210986942052841e-07\n",
      "Test Loss -0.002304365952682495\n",
      " Top 1:  Accuracy: 8887.0/10000 (88.87%)\n",
      "Test Loss: -23.04365952682495\n",
      "EPOCH 9\n",
      "Train Loss kd: 8.834678530693054e-06\n",
      "Test Loss -0.0023684150257110596\n",
      " Top 1:  Accuracy: 8898.0/10000 (88.98%)\n",
      "Test Loss: -23.684150257110595\n",
      "EPOCH 10\n",
      "Train Loss kd: 7.562215924263e-06\n",
      "Test Loss -0.002457544134750366\n",
      " Top 1:  Accuracy: 8910.0/10000 (89.10%)\n",
      "Test Loss: -24.575441347503663\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "count = 0\n",
    "for test, y_test in iter(test_loader):\n",
    "#Calculate the class probabilities (softmax) for img\n",
    "    with torch.no_grad():\n",
    "        output = red_model(test)\n",
    "        ps = torch.exp(output)\n",
    "        _, predicted = torch.max(output.data,1)\n",
    "        total += y_test.size(0)\n",
    "        correct += (predicted == y_test.to(device)).sum().item() #MODIF\n",
    "        count += 1\n",
    "        #print(\"Accuracy of network on test images is {:.4f}....count: {}\".format(100*correct/total,  count ))\n",
    "        if count%50 == 0:\n",
    "            print(\"Accuracy of network on test images is {:.4f}....count: {}\".format(100*correct/total,  count), flush=True)\n",
    "\n",
    "\n",
    "print(\n",
    "      'Pre nnz = {:.2f}, proj_model nnz={:.2f}, FNN nnz={:.4f}'.format(\n",
    "      rednet_storage[0], rednet_storage[1],\n",
    "      rednet_storage[2]), flush=True)\n",
    "print(\n",
    "      'flops:  Pre = {:.2f}, proj_model = {:.2f}, FNN ={:.2f}'.format(\n",
    "       rednet_flops[0], rednet_flops[1], rednet_flops[2]), flush=True)\n",
    "\n",
    "optimizer = torch.optim.Adam([{\n",
    "            'params': red_model.premodel.parameters(),\n",
    "            'lr': 1e-4\n",
    "            }, {\n",
    "            'params': red_model.proj_model.parameters(),\n",
    "            'lr': 1e-5\n",
    "            }, {\n",
    "            'params': red_model.inout_map.parameters(),\n",
    "            'lr': 1e-5\n",
    "            }])\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_loss.append(compute_loss(red_model, device, train_loader))\n",
    "test_loss.append(compute_loss(red_model, device, test_loader))\n",
    "\n",
    "        \n",
    "epochs = 10\n",
    "filename = './cifar10_VGG16_RedNet'+red_method+\\\n",
    "            '_cutIDx_%d.pth'%(cutoff_idx)\n",
    "\n",
    "\"\"\" if os.path.isfile(filename):\n",
    "    [rednet_pretrained, train_loss,test_loss] = torch.load(filename)\n",
    "    red_model.load_state_dict(rednet_pretrained)\n",
    "    print('rednet trained {} epoches is loaded'.format(epochs), flush=True)\n",
    "else:\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    train_loss.append(compute_loss(red_model, device, train_loader))\n",
    "    test_loss.append(compute_loss(red_model, device, test_loader))\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print('EPOCH {}'.format(epoch), flush=True)\n",
    "        train_loss.append(\n",
    "                train_kd(red_model,\n",
    "                model,\n",
    "                device,\n",
    "                train_loader,\n",
    "                optimizer,\n",
    "                train_max_batch=200,\n",
    "                alpha=0.1,\n",
    "                temperature=1.,\n",
    "                epoch=epoch))\n",
    "        test_loss.append(compute_loss(red_model, device, test_loader))\n",
    "    torch.save([red_model.state_dict(), train_loss, test_loss], filename) \"\"\"\n",
    "\n",
    "for epoch in range(1, epochs + 1):                       #da qui alla fine era dentro l'else commentato\n",
    "    print('EPOCH {}'.format(epoch), flush=True)\n",
    "    train_loss.append(\n",
    "            train_kd(red_model,\n",
    "            model,\n",
    "            device,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            train_max_batch=200,\n",
    "            alpha=0.1,\n",
    "            temperature=1.,\n",
    "            epoch=epoch))\n",
    "    test_loss.append(compute_loss(red_model, device, test_loader))\n",
    "torch.save([red_model.state_dict(), train_loss, test_loss], filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('reducedcnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c5bf16c94eb6f9341fa612a12f652937166e39821fa969ec7095b77ab48ffd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
