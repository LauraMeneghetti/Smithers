{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction of VGG16\n",
    "In this tutorial we will present how to create a reduced version of VGG16 using the techniques described in the article ''A Dimensionality Reduction Approach for Convolutional Neural Networks'', Meneghetti L., Demo N., Rozza G., https://arxiv.org/abs/2110.09163 (2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of the model\n",
    "First of all we need to load the model we want to use (in this case VGG16) starting from a checkpoint file, i.e. a file containing the status of the model after a training process with a chosen dataset. Here we will use the CIFAR10 dataset, but we will also show how to generalize everythong using a custom dataset.\n",
    "\n",
    "It is important to highlight that the models of VGG-nets implemented in PyTorch (https://pytorch.org/hub/pytorch_vision_vgg/), e.g. \n",
    "````\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True),\n",
    "```\n",
    "are models pre-trained on the ImageNet dataset, that consists of images of dimensions 224x224. Therefore, in order to use datasets like the CIFAR10, composed of images 32x32, we need to change the architecture of VGG-nets, as was done in the file 'vgg.py'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from smithers.ml.vgg import VGG\n",
    "from smithers.ml.utils import get_seq_model\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "\n",
    "pretrained = '../checkpoint_vgg16_custom20.pth.tar'\n",
    "#model = VGG(None, classifier='cifar', num_classes=4, init_weights=False, pretrain_weights=pretrained)\n",
    "model = torch.load(pretrained)\n",
    "model = model['model']\n",
    "seq_model = get_seq_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of the dataset\n",
    "### CIFAR10 Dataset\n",
    "As stated before, we use the CIFAR10 dataset (already implemented in PyTorch) to test our technique. It is a computer-vision dataset used for object recognition. It consists of 60000 32 Ã— 32 colour images divided in 10 non-overlapping classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.\n",
    "\n",
    "See https://www.cs.toronto.edu/~kriz/cifar.html for more details on this dataset and on how to download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load CIFAR10 dataset for training and testingpretrained = '../checkpoint_vgg16_custom20.pth.tar'\n",
    "batch_size = 8 #this can be changed\n",
    "data_path = '../datasets/' \n",
    "# transform functions: take in input a PIL image and apply this\n",
    "# transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "train_dataset = datasets.CIFAR10(root=data_path + 'CIFAR10/',\n",
    "                                 train=True,\n",
    "                                 download=True,\n",
    "                                 transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_dataset = datasets.CIFAR10(root=data_path + 'CIFAR10/',\n",
    "                                train=False,\n",
    "                                download=True,\n",
    "                                transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "train_labels = torch.tensor(train_loader.dataset.targets)\n",
    "targets = list(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset\n",
    "If we want to use a custom dataset, we need firstly to construct it, following for example the tutorial on the construction of a custom dataset for the problem of Image Recognition. Hence, the previuous cell will be substitute with the following one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data 2759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/lmeneghe/anaconda/envs/prova/lib/python3.8/site-packages/torchvision/transforms/functional.py:135: UserWarning: Legacy tensor constructor is deprecated. Use: torch.tensor(...) for creating tensors from tensor-like objects; or torch.empty(...) for creating an uninitialized tensor with specific sizes. (Triggered internally at  /opt/conda/conda-bld/pytorch_1617865552725/work/torch/csrc/utils/tensor_new.cpp:477.)\n",
      "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from collections import OrderedDict\n",
    "from smithers.ml.imagerec_dataset import Imagerec_Dataset\n",
    "\n",
    "# load custom dataset for training and testing\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "data = pd.read_csv('../dataset_imagerec/dataframe.csv')\n",
    "data_path = '../dataset_imagerec/'\n",
    "# SPLIT OF THE DATASET\n",
    "batch_size = 128\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed = 42\n",
    "\n",
    "dataset_size = len(data)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "print('train data', len(train_indices))\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "resize_dim = [32, 32]\n",
    "\n",
    "dataset_imagerec = Imagerec_Dataset(data, data_path, resize_dim, transform)\n",
    "train_dataset = dataset_imagerec.getdata(train_indices)\n",
    "train_loader = torch.utils.data.DataLoader(dataset_imagerec,\n",
    "                                           batch_size=batch_size,\n",
    "                                           sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_imagerec,\n",
    "                                          batch_size=batch_size,\n",
    "                                          sampler=valid_sampler)\n",
    "\n",
    "classes = ('Cups', 'Dishes', 'Glass', 'Mixed')\n",
    "#classes = ('class_1', 'class_2', 'class_3', 'class_4')\n",
    "n_class = len(classes)\n",
    "targets = list(dataset_imagerec.targets[train_indices])\n",
    "train_labels = torch.tensor(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction of VGG16\n",
    "We now perform the reduction of VGG16 using the module NetAdapter. In this case we use 5 as cut off index and 50 as dimension of the reduced space. For the reduced method and the input-output mapping there are two different choices: 'POD' and 'AS' for the first one, and 'PCE' or 'FNN' for the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of network on test images is 98.4375....count: 1\n",
      "Accuracy of network on test images is 97.6562....count: 2\n",
      "Accuracy of network on test images is 98.1771....count: 3\n",
      "Accuracy of network on test images is 97.8516....count: 4\n",
      "Accuracy of network on test images is 97.9688....count: 5\n",
      "Accuracy of network on test images is 98.1132....count: 6\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "count = 0\n",
    "seq_model.eval()\n",
    "for test, y_test in iter(test_loader):\n",
    "#Calculate the class probabilities (softmax) for img\n",
    "    with torch.no_grad():\n",
    "        output = seq_model(test)\n",
    "        ps = torch.exp(output)\n",
    "        _, predicted = torch.max(output.data,1)\n",
    "        total += y_test.size(0)\n",
    "        correct += (predicted == y_test).sum().item()\n",
    "        count += 1\n",
    "        print(\"Accuracy of network on test images is {:.4f}....count: {}\".format(100*correct/total,  count ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RedNet(\n",
      "  (premodel): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  )\n",
      "  (proj_model): Linear(in_features=4096, out_features=50, bias=False)\n",
      "  (inout_map): FNN(\n",
      "    (fc1): Linear(in_features=50, out_features=20, bias=True)\n",
      "    (fc2): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (fc3): Linear(in_features=20, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from smithers.ml.netadapter import NetAdapter\n",
    "cutoff_idx = 7 \n",
    "red_dim = 50 \n",
    "red_method = 'POD' \n",
    "inout_method = 'FNN'\n",
    "n_class = 4\n",
    "netadapter = NetAdapter(cutoff_idx, red_dim, red_method, inout_method)\n",
    "red_model = netadapter.reduce_net(seq_model, train_dataset, train_labels, train_loader, n_class)\n",
    "print(red_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of network on test images is 28.1250....count: 1\n",
      "Accuracy of network on test images is 25.0000....count: 2\n",
      "Accuracy of network on test images is 23.9583....count: 3\n",
      "Accuracy of network on test images is 24.6094....count: 4\n",
      "Accuracy of network on test images is 26.4062....count: 5\n",
      "Accuracy of network on test images is 25.9797....count: 6\n",
      "Pre nnz = 6.62, proj_model nnz=0.78, FNN nnz=0.0058\n",
      "flops:  Pre = 190.51, proj_model = 0.20, FNN =0.00\n",
      "Test Loss -0.0004433982982572938\n",
      " Top 1:  Accuracy: 674.0/2759 (24.43%)\n",
      "Test Loss: -1.2233359048918737\n",
      "Test Loss -0.0014386674701460749\n",
      " Top 1:  Accuracy: 179.0/689 (25.98%)\n",
      "Test Loss: -0.9912418869306456\n",
      "Test Loss -0.00044339831529596\n",
      " Top 1:  Accuracy: 674.0/2759 (24.43%)\n",
      "Test Loss: -1.2233359519015536\n",
      "Test Loss -0.0014386675846542515\n",
      " Top 1:  Accuracy: 179.0/689 (25.98%)\n",
      "Test Loss: -0.9912419658267793\n",
      "EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/lmeneghe/anaconda/envs/prova/lib/python3.8/site-packages/torch/nn/functional.py:2694: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss kd: 0.000284754115198349\n",
      "Test Loss -0.002672288049586996\n",
      " Top 1:  Accuracy: 468.0/689 (67.92%)\n",
      "Test Loss: -1.8412064661654401\n",
      "EPOCH 2\n",
      "Train Loss kd: 0.000178162539165147\n",
      "Test Loss -0.003921203851779684\n",
      " Top 1:  Accuracy: 527.0/689 (76.49%)\n",
      "Test Loss: -2.701709453876202\n",
      "EPOCH 3\n",
      "Train Loss kd: 0.00015652838631270805\n",
      "Test Loss -0.004585435328241236\n",
      " Top 1:  Accuracy: 559.0/689 (81.13%)\n",
      "Test Loss: -3.1593649411582114\n",
      "EPOCH 4\n",
      "Train Loss kd: 0.00013533468996540884\n",
      "Test Loss -0.005252942879613351\n",
      " Top 1:  Accuracy: 577.0/689 (83.74%)\n",
      "Test Loss: -3.6192776440535988\n",
      "EPOCH 5\n",
      "Train Loss kd: 0.00015816573173769058\n",
      "Test Loss -0.005658894470373372\n",
      " Top 1:  Accuracy: 595.0/689 (86.36%)\n",
      "Test Loss: -3.898978290087253\n",
      "EPOCH 6\n",
      "Train Loss kd: 0.00010584022049110582\n",
      "Test Loss -0.006065483128377559\n",
      " Top 1:  Accuracy: 608.0/689 (88.24%)\n",
      "Test Loss: -4.179117875452138\n",
      "EPOCH 7\n",
      "Train Loss kd: 0.00010270386901874826\n",
      "Test Loss -0.006420820208728109\n",
      " Top 1:  Accuracy: 615.0/689 (89.26%)\n",
      "Test Loss: -4.423945123813668\n",
      "EPOCH 8\n",
      "Train Loss kd: 7.121092644926861e-05\n",
      "Test Loss -0.006735807050536822\n",
      " Top 1:  Accuracy: 626.0/689 (90.86%)\n",
      "Test Loss: -4.64097105781987\n",
      "EPOCH 9\n",
      "Train Loss kd: 7.982549891172868e-05\n",
      "Test Loss -0.006997000474244428\n",
      " Top 1:  Accuracy: 629.0/689 (91.29%)\n",
      "Test Loss: -4.820933326754411\n",
      "EPOCH 10\n",
      "Train Loss kd: 6.49461250626639e-05\n",
      "Test Loss -0.007259532510588292\n",
      " Top 1:  Accuracy: 636.0/689 (92.31%)\n",
      "Test Loss: -5.001817899795333\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from smithers.ml.utils import Total_param, Total_flops\n",
    "from smithers.ml.utils import compute_loss, train_kd\n",
    "\n",
    "rednet_storage = torch.zeros(3)\n",
    "rednet_flops = torch.zeros(3)\n",
    "\n",
    "rednet_storage[0], rednet_storage[1], rednet_storage[2] = [\n",
    "    Total_param(red_model.premodel),\n",
    "    Total_param(red_model.proj_model),\n",
    "    Total_param(red_model.inout_map)]\n",
    "\n",
    "rednet_flops[0], rednet_flops[1], rednet_flops[2] = [\n",
    "    Total_flops(red_model.premodel, device),\n",
    "    Total_flops(red_model.proj_model, device),\n",
    "    Total_flops(red_model.inout_map, device)]\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "count = 0\n",
    "\n",
    "for test, y_test in iter(test_loader):\n",
    "#Calculate the class probabilities (softmax) for img\n",
    "    with torch.no_grad():\n",
    "        output = red_model(test)\n",
    "        ps = torch.exp(output)\n",
    "        _, predicted = torch.max(output.data,1)\n",
    "        total += y_test.size(0)\n",
    "        correct += (predicted == y_test).sum().item()\n",
    "        count += 1\n",
    "        print(\"Accuracy of network on test images is {:.4f}....count: {}\".format(100*correct/total,  count ))\n",
    "\n",
    "print(\n",
    "      'Pre nnz = {:.2f}, proj_model nnz={:.2f}, FNN nnz={:.4f}'.format(\n",
    "      rednet_storage[0], rednet_storage[1],\n",
    "      rednet_storage[2]))\n",
    "print(\n",
    "      'flops:  Pre = {:.2f}, proj_model = {:.2f}, FNN ={:.2f}'.format(\n",
    "       rednet_flops[0], rednet_flops[1], rednet_flops[2]))\n",
    "\n",
    "optimizer = torch.optim.Adam([{\n",
    "            'params': red_model.premodel.parameters(),\n",
    "            'lr': 1e-4\n",
    "            }, {\n",
    "            'params': red_model.proj_model.parameters(),\n",
    "            'lr': 1e-5\n",
    "            }, {\n",
    "            'params': red_model.inout_map.parameters(),\n",
    "            'lr': 1e-5\n",
    "            }])\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_loss.append(compute_loss(red_model, device, train_loader))\n",
    "test_loss.append(compute_loss(red_model, device, test_loader))\n",
    "\n",
    "        \n",
    "epochs = 10\n",
    "filename = './cifar10_VGG16_RedNet'+\\\n",
    "            '_cutIDx_%d.pth'%(cutoff_idx)\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    [rednet_pretrained, train_loss,test_loss] = torch.load(filename)\n",
    "    red_model.load_state_dict(rednet_pretrained)\n",
    "    print('rednet trained {} epoches is loaded'.format(epochs))\n",
    "else:\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    train_loss.append(compute_loss(red_model, device, train_loader))\n",
    "    test_loss.append(compute_loss(red_model, device, test_loader))\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print('EPOCH {}'.format(epoch))\n",
    "        train_loss.append(\n",
    "                train_kd(red_model,\n",
    "                model,\n",
    "                device,\n",
    "                train_loader,\n",
    "                optimizer,\n",
    "                train_max_batch=200,\n",
    "                alpha=0.1,\n",
    "                temperature=1.,\n",
    "                epoch=epoch))\n",
    "        test_loss.append(compute_loss(red_model, device, test_loader))\n",
    "    torch.save([red_model.state_dict(), train_loss, test_loss], filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
