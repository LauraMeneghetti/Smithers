{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD Reduction tutorial\n",
    "\n",
    "In this tutorial, we will show how to initialize, reduce and train a SSD300 network using POD and/or AHOSVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS\n",
    "We start by importing all the necessary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'smithers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-94f37a1979ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msmithers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVGG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msmithers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maux_conv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAuxiliaryConvolutions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msmithers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPredictionConvolutions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'smithers'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Create reduced version of SSD300\n",
    "'''\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "from PIL import Image\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils import data\n",
    "import pickle\n",
    "\n",
    "from smithers.ml.vgg import VGG\n",
    "from smithers.ml.models.aux_conv import AuxiliaryConvolutions\n",
    "from smithers.ml.models.predictor import PredictionConvolutions\n",
    "from smithers.ml.dataset.pascalvoc_dataset import PascalVOCDataset\n",
    "from smithers.ml.models.detector import Detector, Reduced_Detector\n",
    "from smithers.ml.models.utils import create_prior_boxes, save_checkpoint_objdet\n",
    "from smithers.ml.netadapter import NetAdapter\n",
    "from smithers.ml.utils import get_seq_model, Total_param, Total_flops\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEARNING PARAMETERS\n",
    "Then, we set the parameters used for the data, the detector and the learning phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories: {'cat': 1, 'dog': 2, 'background': 0}\n",
      "n_classes: 3\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Learning parameters\n",
    "batch_size = 8  # batch size\n",
    "workers = 4  # number of workers for loading data in the DataLoader\n",
    "iterations = 120000  # number of iterations to train\n",
    "print_freq = 200  # print training status every __ batches\n",
    "lr = 1e-4  # learning rate\n",
    "decay_lr_at = [80000, 100000]  # decay learning rate after these many iterations\n",
    "decay_lr_to = 0.1\n",
    "# decay learning rate to this fraction of the existing learning rate\n",
    "#n_classes = 6\n",
    "momentum = 0.9  # momentum\n",
    "weight_decay = 5e-4  # weight decay\n",
    "grad_clip = None\n",
    "# clip if gradients are exploding, which may happen at larger batch sizes\n",
    "\n",
    "voc_labels = ('cat', 'dog')\n",
    "'''voc_labels = ('aeroplane', 'bicycle', 'bird', 'boat',\n",
    "        'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "        'cow', 'diningtable', 'dog', 'horse',\n",
    "        'motorbike', 'person', 'pottedplant',\n",
    "        'sheep', 'sofa', 'train', 'tvmonitor')'''\n",
    "label_map = {k: v + 1 for v, k in enumerate(voc_labels)}\n",
    "label_map['background'] = 0\n",
    "n_classes = len(label_map)\n",
    "print('categories:',label_map)\n",
    "print('n_classes:', n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PARAMETERS\n",
    "We now define the train and test images, after they have been extracted and the subdivision has been carried out (refer to the data preparation tutorial for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 240\n",
      "Testing images: 60\n"
     ]
    }
   ],
   "source": [
    "# Data parameters\n",
    "data_folder = 'VOC_dog_cat/JSONfiles' #folder with json data files\n",
    "keep_difficult = True\n",
    "\n",
    "\n",
    "train_dataset = PascalVOCDataset(data_folder,\n",
    "                                 split='train',\n",
    "                                 keep_difficult=keep_difficult)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=train_dataset.collate_fn,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True)\n",
    "\n",
    "epochs = iterations // (len(train_dataset) // 32)\n",
    "decay_lr_at = [it // (len(train_dataset) // 32) for it in decay_lr_at]\n",
    "print('Training images:', len(train_dataset))\n",
    "# Load test data\n",
    "test_dataset = PascalVOCDataset(data_folder,\n",
    "                                split='test',\n",
    "                                keep_difficult=keep_difficult)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          collate_fn=test_dataset.collate_fn,\n",
    "                                          num_workers=workers,\n",
    "                                          pin_memory=True)\n",
    "print('Testing images:', len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REDUCTION VIA AHOSVD\n",
    "The following block has been set up for carrying out the reduction using the AHOSVD technique. The fundamental parameter in this context is the list ``mode_list_batch``, which is composed as follows:\n",
    "- the first element, at position ``0``, is the number of images that get processed together in the HOSVD\n",
    "- the latter three numbers define, respectively, the number of channels, the height and the width of the output of this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded base model.\n",
      "\n",
      "Initializing reduction. Chosen reduction method is: HOSVD\n",
      "Initializing dataset forwarding (AHOSVD included)\n",
      "30\n",
      "completato\n",
      "Dataset forwarding (with AHOSVD reduction) complete\n",
      "time needed to initialize the model 78.11 seconds\n"
     ]
    }
   ],
   "source": [
    "#checkpoint = 'checkpoint_ssd300.pth.tar'\n",
    "init_time = time()\n",
    "\n",
    "base_net = VGG(classifier='ssd', init_weights=False)\n",
    "seq_model = get_seq_model(base_net)\n",
    "cutoff_idx = 7\n",
    "red_method = 'HOSVD'\n",
    "if red_method == 'POD':\n",
    "    red_dim = 50\n",
    "elif red_method == 'HOSVD':\n",
    "    mode_list_batch=[1, 35, 3, 3]\n",
    "    red_dim = mode_list_batch[-1]*mode_list_batch[-1]*mode_list_batch[1]\n",
    "inout_method = None\n",
    "netadapter = NetAdapter(cutoff_idx, red_dim, red_method, inout_method)\n",
    "red_model = netadapter.reduce_net(seq_model, train_dataset, None, train_loader, n_classes, device = device, mode_list_batch = mode_list_batch)\n",
    "base_net = red_model.premodel\n",
    "aux_conv = red_model.proj_model\n",
    "#cfg_tot = [n of channels of the outputs of the pre model, n of the reduced channels]\n",
    "if red_method == 'HOSVD':\n",
    "    cfg_tot = list(reversed(list(red_model.proj_model.list_of_matrices[0].shape)))\n",
    "elif red_method == 'POD':\n",
    "    cfg_tot = [256, red_dim]\n",
    "#cfg_tot_ssd = [512,1024,] #channel number\n",
    "n_boxes = [4, 6]\n",
    "predictor = PredictionConvolutions(n_classes, cfg_tot, n_boxes)\n",
    "network = [base_net, aux_conv, predictor]\n",
    "\n",
    "#create prior boxes custom for reduced net\n",
    "if red_method == 'HOSVD':\n",
    "    reduction_dims = list(reversed(list(red_model.proj_model.list_of_matrices[1].shape)))\n",
    "    #fmaps_dims = {'premodel': width=height of the output of the pre model, 'projmodel': width=height of the reduced tensors}\n",
    "    fmaps_dims = {'premodel': reduction_dims[0], 'projmodel': reduction_dims[1]}\n",
    "elif red_method == 'POD':\n",
    "    fmaps_dims = {'premodel': 38, 'projmodel': 1}\n",
    "obj_scales = {'premodel': 0.1, 'projmodel': 0.725} #0.9\n",
    "aspect_ratio = {'premodel': [1., 2., 0.5], 'projmodel': [1., 2., 3., 0.5, 0.333]}\n",
    "priors_cxcy = create_prior_boxes(fmaps_dims, obj_scales, aspect_ratio)\n",
    "init_end = time()\n",
    "print('time needed to initialize the model', round(init_end - init_time,2), 'seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REDUCTION VIA POD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded base model.\n",
      "\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU(inplace=True)\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace=True)\n",
      "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): ReLU(inplace=True)\n",
      "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU(inplace=True)\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU(inplace=True)\n",
      "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (25): ReLU(inplace=True)\n",
      "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (27): ReLU(inplace=True)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace=True)\n",
      "  (30): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "  (31): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (32): Flatten(start_dim=1, end_dim=-1)\n",
      "  (33): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
      "  (34): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "Initializing reduction. Chosen reduction method is: POD\n",
      "Initializing dataset forwarding\n",
      "Dataset forwarding complete\n",
      "RedNet(\n",
      "  (premodel): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  )\n",
      "  (proj_model): Linear(in_features=369664, out_features=50, bias=False)\n",
      "  (inout_map): Identity()\n",
      ")\n",
      "Linear(in_features=369664, out_features=50, bias=False)\n",
      "time needed to initialize the model 6.261075735092163\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoint_ssd300.pth.tar'\n",
    "checkpoint = None\n",
    "init_time = time()\n",
    "\n",
    "base_net = VGG(classifier='ssd', init_weights=False)\n",
    "seq_model = get_seq_model(base_net)\n",
    "print(seq_model)\n",
    "cutoff_idx = 7\n",
    "red_dim = 50\n",
    "red_method = 'POD'\n",
    "inout_method = None\n",
    "netadapter = NetAdapter(cutoff_idx, red_dim, red_method, inout_method)\n",
    "red_model = netadapter.reduce_net(seq_model, train_dataset, None, train_loader, n_classes)\n",
    "print(red_model)\n",
    "base_net = red_model.premodel\n",
    "aux_conv = red_model.proj_model\n",
    "print(aux_conv)\n",
    "cfg_tot = [256, 50] #, 512, 256, 256, 256]\n",
    "n_boxes = [4, 6]\n",
    "predictor = PredictionConvolutions(n_classes, cfg_tot, n_boxes)\n",
    "network = [base_net, aux_conv, predictor]\n",
    "\n",
    "#create prior boxes custom for reduced net\n",
    "fmaps_dims = {'premodel': 38, 'projmodel': 1} \n",
    "obj_scales = {'premodel': 0.1, 'projmodel': 0.725} #0.9\n",
    "aspect_ratio = {'premodel': [1., 2., 0.5], 'projmodel': [1., 2., 3., 0.5, 0.333]}\n",
    "priors_cxcy = create_prior_boxes(fmaps_dims, obj_scales, aspect_ratio)\n",
    "\n",
    "init_end = time()\n",
    "print('time needed to initialize the model', init_end - init_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING THE REDUCED NETWORK\n",
    "The following cell deals with the training of the reduced network. It is possible to either load an existing checkpoint and continue the training of it or start from a newly initialized network. As of now, the training phase will save a checkpoint every 500 epochs of training and after its completion. This parameter can be adjusted in the ``train_detector_with_eval_name`` function, that can be found in the detector.py script.\n",
    "\n",
    "The second part of the cell measures the time needed for the network to make a single predicition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (with evaluation) has started.\n",
      "Epoch: [0][0/30]\tBatch Time 0.468 (0.468)\tData Time 0.298 (0.298)\tLoss 5.4450 (5.4450)\t\n",
      "\n",
      "Mean Average Precision (mAP): 0.001\n",
      "Epoch: [1][0/30]\tBatch Time 0.421 (0.421)\tData Time 0.274 (0.274)\tLoss 5.2341 (5.2341)\t\n",
      "\n",
      "Mean Average Precision (mAP): 0.000\n",
      "Epoch: [2][0/30]\tBatch Time 0.377 (0.377)\tData Time 0.242 (0.242)\tLoss 5.3344 (5.3344)\t\n",
      "\n",
      "Mean Average Precision (mAP): 0.000\n",
      "Epoch: [3][0/30]\tBatch Time 0.419 (0.419)\tData Time 0.265 (0.265)\tLoss 5.2606 (5.2606)\t\n",
      "\n",
      "Mean Average Precision (mAP): 0.000\n",
      "Epoch: [4][0/30]\tBatch Time 0.456 (0.456)\tData Time 0.288 (0.288)\tLoss 5.3757 (5.3757)\t\n",
      "\n",
      "Mean Average Precision (mAP): 0.000\n",
      "Epoch: [5][0/30]\tBatch Time 0.363 (0.363)\tData Time 0.227 (0.227)\tLoss 5.2238 (5.2238)\t\n",
      "\n",
      "Mean Average Precision (mAP): 0.000\n",
      "Epoch: [6][0/30]\tBatch Time 0.443 (0.443)\tData Time 0.284 (0.284)\tLoss 5.2474 (5.2474)\t\n",
      "\n",
      "Mean Average Precision (mAP): 0.000\n",
      "Epoch: [7][0/30]\tBatch Time 0.442 (0.442)\tData Time 0.282 (0.282)\tLoss 5.3080 (5.3080)\t\n",
      "\n",
      "Mean Average Precision (mAP): 0.002\n",
      "Epoch: [8][0/30]\tBatch Time 0.391 (0.391)\tData Time 0.258 (0.258)\tLoss 5.3895 (5.3895)\t\n",
      "\n",
      "Mean Average Precision (mAP): 0.000\n",
      "Epoch: [9][0/30]\tBatch Time 0.408 (0.408)\tData Time 0.275 (0.275)\tLoss 5.3717 (5.3717)\t\n",
      "\n",
      "Mean Average Precision (mAP): 0.000\n",
      "Training (with evaluation) is now complete.\n",
      "Time needed for training: 185.86 seconds, i.e. 3.1 minutes\n"
     ]
    }
   ],
   "source": [
    "check = None\n",
    "#check = 'Results/3000_2_55_3_3_cut11/checkpoint_ssd300.pth.tar'\n",
    "epochs = 10\n",
    "start = time()\n",
    "detector = Reduced_Detector(network, check, priors_cxcy, n_classes, epochs,\n",
    "                    batch_size, print_freq, lr, decay_lr_at,\n",
    "                    decay_lr_to, momentum, weight_decay, grad_clip,\n",
    "                    train_loader, test_loader)\n",
    "\n",
    "start = time()\n",
    "check, loss_value, mAP_list = detector.train_detector_with_eval(label_map)\n",
    "end = time()\n",
    "print(f'Time needed for training: {round(end-start,2)} seconds, i.e. {round((end-start)/60,1)} minutes')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACCURACY OF AN EXISTING NETWORK\n",
    "With the following piece of code, it is possible to load a checkpoint and check its accuracy on the test dataset initially defined.\n",
    "Moreover, the second part of the cell is used to test the network on a user given picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded checkpoint from epoch 10.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 8/8 [00:14<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': 0.00010449321416672319, 'dog': 0.0}\n",
      "{'cat': 0.00010449321416672319, 'dog': 0.0}\n",
      "\n",
      "Mean Average Precision (mAP): 0.000\n",
      "Time needed for testing: 14.53 seconds, i.e. 0.2 minutes\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/u/s/szanin/Smithers/smithers/ml/tutorials/VOC_dog_cat/JPEGImages/001463.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/u/s/szanin/Smithers/smithers/ml/tutorials/reduction_SSD.ipynb Cella 15\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bagnesi/u/s/szanin/Smithers/smithers/ml/tutorials/reduction_SSD.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTime needed for testing: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mround\u001b[39m(end_test\u001b[39m-\u001b[39mstart_test,\u001b[39m2\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m seconds, i.e. \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mround\u001b[39m((end_test\u001b[39m-\u001b[39mstart_test)\u001b[39m/\u001b[39m\u001b[39m60\u001b[39m,\u001b[39m1\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m minutes\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bagnesi/u/s/szanin/Smithers/smithers/ml/tutorials/reduction_SSD.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m img_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/u/s/szanin/Smithers/smithers/ml/tutorials/VOC_dog_cat/JPEGImages/001463.jpg\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bagnesi/u/s/szanin/Smithers/smithers/ml/tutorials/reduction_SSD.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m original_image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(img_path, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bagnesi/u/s/szanin/Smithers/smithers/ml/tutorials/reduction_SSD.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m original_image \u001b[39m=\u001b[39m original_image\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bagnesi/u/s/szanin/Smithers/smithers/ml/tutorials/reduction_SSD.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m detector\u001b[39m.\u001b[39mdetect(original_image,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bagnesi/u/s/szanin/Smithers/smithers/ml/tutorials/reduction_SSD.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m                 check,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bagnesi/u/s/szanin/Smithers/smithers/ml/tutorials/reduction_SSD.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m                 label_map,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bagnesi/u/s/szanin/Smithers/smithers/ml/tutorials/reduction_SSD.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m                 min_score\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bagnesi/u/s/szanin/Smithers/smithers/ml/tutorials/reduction_SSD.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m                 max_overlap\u001b[39m=\u001b[39m\u001b[39m0.45\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bagnesi/u/s/szanin/Smithers/smithers/ml/tutorials/reduction_SSD.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m                 top_k\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/miniconda3/envs/reducedcnn/lib/python3.10/site-packages/PIL/Image.py:3092\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3089\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[1;32m   3091\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[0;32m-> 3092\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   3093\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   3095\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/u/s/szanin/Smithers/smithers/ml/tutorials/VOC_dog_cat/JPEGImages/001463.jpg'"
     ]
    }
   ],
   "source": [
    "#check = None\n",
    "#check = 'Results/Ulysses/3200_3_35_3_3/epoch_1700-3200/checkpoint_ssd300_epoch_2000.pth.tar'\n",
    "epochs = 1\n",
    "start = time()\n",
    "detector = Reduced_Detector(network, check, priors_cxcy, n_classes, epochs,\n",
    "                    batch_size, print_freq, lr, decay_lr_at,\n",
    "                    decay_lr_to, momentum, weight_decay, grad_clip,\n",
    "                    train_loader, test_loader)\n",
    "start_test = time()\n",
    "detector.eval_detector(label_map, check)\n",
    "end_test = time()\n",
    "print(f'Time needed for testing: {round(end_test-start_test,2)} seconds, i.e. {round((end_test-start_test)/60,1)} minutes')\n",
    "\n",
    "\n",
    "\n",
    "img_path = '/u/s/szanin/Smithers/smithers/ml/tutorials/VOC_dog_cat/JPEGImages/001462.jpg'\n",
    "original_image = Image.open(img_path, mode='r')\n",
    "original_image = original_image.convert('RGB')\n",
    "detector.detect(original_image,\n",
    "                check,\n",
    "                label_map,\n",
    "                min_score=0.01,\n",
    "                max_overlap=0.45,\n",
    "                top_k=5).show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STORAGE SIZE OF AN EXISTING (REDUCED) NETWORK\n",
    "With the following piece of code, it is possible to load a checkpoint of a reduced network and check its storage size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSD300 reduced-storage\n",
      " Pre nnz = 38.13, POD_model nnz=0.11, feature_loc nnz=0.3267, feature_cl nnz=0.2450\n"
     ]
    }
   ],
   "source": [
    "#check = 'checkpoint_ssd300_red_pascalvoc.pth.tar'\n",
    "#check = 'checkpoint_ssd300.pth.tar'\n",
    "check = torch.load(check)\n",
    "model = check['model']\n",
    "\n",
    "rednet_storage = torch.zeros(4)\n",
    "rednet_flops = torch.zeros(4)\n",
    "\n",
    "rednet_storage[0], rednet_storage[1], rednet_storage[2], rednet_storage[3] = [\n",
    "       Total_param(model[0]),\n",
    "       Total_param(model[1]),\n",
    "       Total_param(model[2].features_loc),\n",
    "       Total_param(model[2].features_cl)]\n",
    "\n",
    "print('SSD300 reduced-storage')\n",
    "print(\n",
    "      ' Pre nnz = {:.2f}, POD_model nnz={:.2f}, feature_loc nnz={:.4f}, feature_cl nnz={:.4f}'.format(\n",
    "                  rednet_storage[0], rednet_storage[1],\n",
    "                  rednet_storage[2], rednet_storage[3]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STORAGE SIZE OF AN EXISTING (UNREDUCED) NETWORK\n",
    "With the following piece of code, it is possible to load a checkpoint of a unreduced SSD300 network and check its storage size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSD300-storage\n",
      " Pre nnz = 78.14, aux_model nnz=9.38, feature_loc nnz=2.0395, feature_cl nnz=1.5296\n",
      " Pre nnz = 78.14, pre_vgg nnz=56.13, pre_avgpool nnz=0.0000, pre_classifier nnz=22.0078\n"
     ]
    }
   ],
   "source": [
    "check1 = 'checkpoint_ssd300.pth.tar'\n",
    "check1 = torch.load(check1)\n",
    "model = check1['model']\n",
    "\n",
    "rednet_storage = torch.zeros(4)\n",
    "rednet_flops = torch.zeros(4)\n",
    "\n",
    "rednet_storage[0], rednet_storage[1], rednet_storage[2], rednet_storage[3]  = [\n",
    "       Total_param(model[0]),\n",
    "       Total_param(model[1].features),\n",
    "       Total_param(model[2].features_loc),\n",
    "       Total_param(model[2].features_cl)]\n",
    "\n",
    "rednet_vgg_storage = torch.zeros(4)\n",
    "rednet_vgg_storage[0], rednet_vgg_storage[1], rednet_vgg_storage[2], rednet_vgg_storage[3],  = [\n",
    "       Total_param(model[0]),\n",
    "       Total_param(model[0].features),\n",
    "       Total_param(model[0].avgpool),\n",
    "       Total_param(model[0].classifier)]\n",
    "\n",
    "\n",
    "print('SSD300-storage')\n",
    "print(\n",
    "      ' Pre nnz = {:.2f}, aux_model nnz={:.2f}, feature_loc nnz={:.4f}, feature_cl nnz={:.4f}'.format(\n",
    "                  rednet_storage[0], rednet_storage[1],\n",
    "                  rednet_storage[2], rednet_storage[3]))\n",
    "\n",
    "\"\"\"print(\n",
    "      ' Pre nnz = {:.2f}, pre_vgg nnz={:.2f}, pre_avgpool nnz={:.4f}, pre_classifier nnz={:.4f}'.format(\n",
    "                  rednet_vgg_storage[0], rednet_vgg_storage[1],\n",
    "                  rednet_vgg_storage[2], rednet_vgg_storage[3]))\"\"\"\n",
    "\n",
    "\n",
    "torch.save(detector, 'check_ssd300_red.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c5bf16c94eb6f9341fa612a12f652937166e39821fa969ec7095b77ab48ffd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
